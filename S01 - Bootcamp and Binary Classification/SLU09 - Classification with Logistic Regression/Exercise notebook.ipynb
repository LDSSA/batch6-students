{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c8e2c44",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3c6bb5ea4054a12b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# SLU09 - Classification With Logistic Regression: Exercise notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84735bf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ceecbe97ab3fbc07",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import hashlib\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9951c5d8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-424f9028a0d6b937",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You thought that you would get away without implementing your own little logistic regression? Hah! In this notebook, you will:\n",
    "- implement one pass of maximum likelihood optimization in three steps: implement the estimated probability function, calculate the log-likelihood cost function, and calculate one iteration of the optimization\n",
    "- standardize data manually\n",
    "- use sklearn for the same steps: standardize data, train the classifier and output predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7342d1d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cad449d76d75cf33",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.1: Calculate the estimated probability\n",
    "\n",
    "Recall the formula for the estimated probability for logistic regression:\n",
    "\n",
    "$$\\hat{p} = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Where z is the linear combination of the features $x_n$ and $\\beta_n$ are the coefficients of the model:\n",
    "\n",
    "$$z = \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n$$\n",
    "\n",
    "Implement a function that calculates the estimated probability for an observation. The input are two arrays, one with the features (x1, x2, ..., xn) and another with the model coefficients (b0, b1, .., bn). The output is the estimated probability for the given observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca472f1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1655212e9fba3a56",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def predict_proba(data, coefs):\n",
    "    \"\"\" \n",
    "    Function that returns the estimated probability for an observation.\n",
    "    \n",
    "    Args:\n",
    "        data (np.array): a numpy array of shape (n) with the features\n",
    "        coefs (np.array): a numpy array of shape (n + 1, 1) with model coefficients\n",
    "            - coefs[0]: intercept\n",
    "            - coefs[1:]: remaining coefficients\n",
    "\n",
    "    Returns:\n",
    "        proba (float): the estimated probability, value between 0 and 1.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # hint: if using array multiplication, don't forget to add a field \n",
    "    #       for the intercept to the features (like you did in SLU07)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f7e3ef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1eb857ba361aef74",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([-1.2, -1.5])\n",
    "coefficients = np.array([0 ,4, -1])\n",
    "np.testing.assert_almost_equal(round(predict_proba(x, coefficients),3),0.036)\n",
    "\n",
    "x_1 = np.array([-1.5, -1, 3, 0])\n",
    "coefficients_1 = np.array([0 ,2.1, -1, 0.5, 0])\n",
    "np.testing.assert_almost_equal(round(predict_proba(x_1, coefficients_1),3),0.343)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77149736",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b7c129fec644b95c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.2: Compute the log-likelihood cost function\n",
    "\n",
    "During the optimization of the model coefficients, you need to calculate the log-likelihood cost function: \n",
    "\n",
    "$$H_{\\hat{p}}(y) = \\sum_{i=1}^{N} \\left [ y_i \\log\\left(\\hat{p}_i(x_i,\\beta)\\right) + (1-y_i) \\log\\left(1-\\hat{p}_i(x_i,\\beta)\\right) \\right ]$$\n",
    "\n",
    "where N is the number of observations, $y_i$ are the true class labels, $x_i$ the feature vector of the ith observation, and $\\beta$ are the model coefficients.\n",
    "\n",
    "In this exercise, you will calculate the cost function for the given dataset. The inputs are an array of the feature vectors, an array of the model coefficients, and an array of the true class labels. You can use the function above or calculate everything from scratch, in which case it will be easier if you still remember how to multiply matrices. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921cbe33",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f7e2f1ca03e6eda0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def log_likelihood_cost_function(var_x, coefs, var_y):\n",
    "    \"\"\" \n",
    "    Function that calculates log-likelihood for the given dataset\n",
    "    \n",
    "    Args:\n",
    "        var_x (np.array): array with the features of the training data of size (m, n)\n",
    "                   where m is the number of observations and n the number of features\n",
    "        coefs (float64): an array with the model coefficients of size (1, n+1)\n",
    "        var_y (float64): an array with the true class labels of size (m,1)\n",
    "        \n",
    "    Returns:\n",
    "        cost (np.float): a float with the resulting log-likelihood for the dataset\n",
    "\n",
    "    \"\"\"\n",
    "   \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d523045d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5ae176d7cc3254aa",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[-2, -2], [3.5, 0], [6, 4]])\n",
    "coefficients = np.array([[0 ,2, -1]])\n",
    "y = np.array([[1],[1],[0]])\n",
    "np.testing.assert_almost_equal(round(log_likelihood_cost_function(x, coefficients, y),2),-10.13,1)\n",
    "coefficients_1 = np.array([[3 ,4, -0.6]])\n",
    "x_1 = np.array([[-4, -4], [6, 0], [3, 2], [4, 0]])\n",
    "y_1 = np.array([[0],[1],[0],[1]])\n",
    "np.testing.assert_almost_equal(round(log_likelihood_cost_function(x_1, coefficients_1, y_1),2),-13.8,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab3664",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-88110f287afd1190",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.3: Compute one iteration of the gradient descent\n",
    "\n",
    "Now that we know how to calculate probabilities and the cost function, let's do an interesting exercise - compute the first iteration of the gradient descent for the given dataset according to the update rule\n",
    "\n",
    "$$\\beta_{t+1} = \\beta_t + learning\\_rate*\\sum_{i=1}^{N}  \\left[ x_i \\left(y_i-\\hat{p}_i(x_i,\\beta_t)\\right) \\right] $$\n",
    "\n",
    "Write a function that takes as arguments the training data and the learning rate and outputs the model coefficients $\\beta$ after one iteration of the gradient descent. Initialize the coefficients with 0 like this:\n",
    "```python\n",
    "coefficients = np.zeros(n+1)\n",
    "```\n",
    "where n is the number of features of the model. Before you start, think for a moment about the dimensions of the terms in the sum that you need to multiply. Writing it down on paper helps. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8379ea45",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9c055d55276e6219",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_coefs_gd(x_train, y_train, learning_rate = 0.1, verbose = False):\n",
    "    \"\"\" \n",
    "    Function that calculates the logistic regression coefficients \n",
    "    after the first iteration of gradient descent.\n",
    "\n",
    "    Args:\n",
    "        x_train (np.array): a numpy array with features of shape (m, n)\n",
    "            m: number of training observations\n",
    "            n: number of features\n",
    "        y_train (np.array): a numpy array with the true class labels of shape (m,)\n",
    "        learning_rate (np.float64): learning rate for the optimization\n",
    "\n",
    "    Returns:\n",
    "        coefficients (np.array): a numpy array of updated model coefficients of shape (n+1,)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da8d12d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ca37dc488930bc79",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Test 1\n",
    "x_train = np.array([[5.5,2.3,4.0,1.3], [6.9,3.1,4.9,1.5], [7.3,2.9,6.3,1.8], [6.4,2.8,5.6,2.1]])\n",
    "y_train = np.array([0,0,1,1])\n",
    "learning_rate = 0.1\n",
    "x_standard=StandardScaler().fit_transform(x_train)\n",
    "coef=compute_coefs_gd(x_standard, y_train, learning_rate)\n",
    "\n",
    "np.testing.assert_almost_equal(round(coef[0],3),0)\n",
    "np.testing.assert_almost_equal(round(coef[1],3),0.097)\n",
    "np.testing.assert_almost_equal(round(coef[2],3),0.051)\n",
    "np.testing.assert_almost_equal(round(coef[3],3),0.176)\n",
    "np.testing.assert_almost_equal(round(coef[4],3),0.181)\n",
    "\n",
    "#Test 2\n",
    "x_train_1 = np.array([[6.7,3.0,5.2,2.3], [6.3,2.5,5.0,1.9], [7.7,3.8,6.7,2.2], [7.7,2.6,6.9,2.3],\n",
    "                      [6.0,2.7,5.1,1.6], [5.4,3.0,4.5,1.5], [6.3,3.3,4.7,1.6], [4.9,2.4,3.3,1.0]])\n",
    "y_train_1 = np.array([0,0,0,0,1,1,1,1])\n",
    "learning_rate = 0.1\n",
    "x_1_standard=StandardScaler().fit_transform(x_train_1)\n",
    "coef1=compute_coefs_gd(x_1_standard, y_train_1, learning_rate)\n",
    "\n",
    "np.testing.assert_almost_equal(round(coef1.max(),3) ,0.)\n",
    "np.testing.assert_almost_equal(round(coef1.min(),3) ,-0.349)\n",
    "np.testing.assert_almost_equal(round(coef1.mean(),3),-0.2)\n",
    "np.testing.assert_almost_equal(round(coef1.var(),3) ,0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2149541",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9a23cad21f43aa24",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.4: Compute one iteration of Newton's method\n",
    "\n",
    "After you mastered the previous exercise, this one will be a breeze. Do one iteration for Newton's method using the update rule\n",
    "\n",
    "$$\\beta_{t+1} = \\beta_t + learning\\_rate * \\sum_{i=1}^{N} \\frac{\\left(y_i - \\hat{p}_i(x_i,\\beta_t)\\right)} {\\hat{p}_i(x_i,\\beta_t) \\ \\left(1 - \\hat{p}_i(x_i,\\beta_t)\\right) \\ x_i }$$\n",
    "\n",
    "Write a function that takes as arguments the training data and the learning rate and outputs the model coefficients $\\beta$ after one iteration of Newton's method. Initialize the coefficients with 0 as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2acb21",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5953d9a8da68ac35",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_coefs_nm(x_train, y_train, learning_rate = 0.1, verbose = False):\n",
    "    \"\"\" \n",
    "    Function that calculates the logistic regression coefficients \n",
    "    after the first iteration of Newton's method.\n",
    "\n",
    "    Args:\n",
    "        x_train (np.array): a numpy array with features of shape (m, n)\n",
    "            m: number of training observations\n",
    "            n: number of features\n",
    "        y_train (np.array): a numpy array with the true class labels of shape (m,)\n",
    "        learning_rate (np.float64): learning rate for the optimization\n",
    "\n",
    "    Returns:\n",
    "        coefficients (np.array): a numpy array of updated model coefficients of shape (n+1,)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49940f2e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-26894414e8e4d657",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Test 1\n",
    "x_train = np.array([[5.5,2.3,4.0,1.3], [6.9,3.1,4.9,1.5], [7.3,2.9,6.3,1.8], [6.4,2.8,5.6,2.1]])\n",
    "y_train = np.array([0,0,1,1])\n",
    "learning_rate = 0.1\n",
    "x_standard=StandardScaler().fit_transform(x_train)\n",
    "coef=compute_coefs_nm(x_standard, y_train, learning_rate)\n",
    "\n",
    "np.testing.assert_almost_equal(round(coef[0],3),0)\n",
    "np.testing.assert_almost_equal(round(coef[1],3),-1.129)\n",
    "np.testing.assert_almost_equal(round(coef[2],3),2.772)\n",
    "np.testing.assert_almost_equal(round(coef[3],3),1.29)\n",
    "np.testing.assert_almost_equal(round(coef[4],3),1.136)\n",
    "\n",
    "#Test 2\n",
    "x_train_1 = np.array([[6.7,3.0,5.2,2.3], [6.3,2.5,5.0,1.9], [7.7,3.8,6.7,2.2], [7.7,2.6,6.9,2.3],\n",
    "                      [6.0,2.7,5.1,1.6], [5.4,3.0,4.5,1.5], [6.3,3.3,4.7,1.6], [4.9,2.4,3.3,1.0]])\n",
    "y_train_1 = np.array([0,0,0,0,1,1,1,1])\n",
    "learning_rate = 0.1\n",
    "x_1_standard=StandardScaler().fit_transform(x_train_1)\n",
    "coef1=compute_coefs_nm(x_1_standard, y_train_1, learning_rate)\n",
    "\n",
    "np.testing.assert_almost_equal(round(coef1.max(),3) ,0.037)\n",
    "np.testing.assert_almost_equal(round(coef1.min(),3) ,-11.567)\n",
    "np.testing.assert_almost_equal(round(coef1.mean(),3),-3.173)\n",
    "np.testing.assert_almost_equal(round(coef1.var(),3) ,18.671)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3510f3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d63b1a6f88d1d3b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2: Standardize data\n",
    "\n",
    "To get this concept in your head, let's do a quick and easy function to standardize the data. Recall that standardized data have zero mean and unit variance:\n",
    "\n",
    "$$ x_{standardized} = \\frac{x - mean(x)}{std(x)}$$\n",
    "\n",
    "Don't forget that the `axis` argument is critical when obtaining the mean values!\n",
    "\n",
    "Implement the function to standardize given data below. Inputs is an array of features and output is an array of the same size with standardized features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb924599",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-842c59e2cd1b9d23",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def standardize_data_function(data):\n",
    "    \"\"\" \n",
    "    Function that standardizes the features\n",
    "    \n",
    "    Args:\n",
    "        data (np.array): a numpy array with observations of shape (m, n)\n",
    "            m: number of observations\n",
    "            n: number of features\n",
    "\n",
    "    Returns:\n",
    "        standardized_data (np.array): a numpy array with standardized features of shape (m, n)\n",
    "\n",
    "    \"\"\"\n",
    "   \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return standardized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f815ed",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1dbfeab7e0a5f336",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "data = np.array([[7,7,3], [2,2,11], [9,5,2], [0,9,5], [10,1,3], [1,5,2]])\n",
    "standardized_data = standardize_data_function(data)\n",
    "print('Before standardization:')\n",
    "print(data)\n",
    "print('\\n-------------------\\n')\n",
    "print('After standardization:')\n",
    "print(standardized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8098343",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8bfbe2697edf51e3",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "data = np.array([[2,2,11,1], [7,5,1,3], [9,5,2,6]])\n",
    "standardized_data = standardize_data_function(data)\n",
    "np.testing.assert_almost_equal(round(standardized_data.mean(),0),0.)\n",
    "np.testing.assert_almost_equal(round(standardized_data.var(axis=0).mean(),0),1.)\n",
    "np.testing.assert_almost_equal(round(standardized_data.min(),3),-1.414)\n",
    "np.testing.assert_almost_equal(round(standardized_data.max(),3),1.408)\n",
    "\n",
    "data1 = np.array([[1,3,1,3], [9,5,3,1], [2,2,4,6]])\n",
    "standardized_data1 = standardize_data_function(data1)\n",
    "np.testing.assert_almost_equal(round(standardized_data1.mean(),0),0.)\n",
    "np.testing.assert_almost_equal(round(standardized_data1.var(axis=0).mean(),0),1.)\n",
    "np.testing.assert_almost_equal(round(standardized_data1.min(),3),-1.336)\n",
    "np.testing.assert_almost_equal(round(standardized_data1.max(),3),1.405)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e383b0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c9ee39c1b9a7f3e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 3.1: Train a logistic regression classifier with sklearn\n",
    "\n",
    "Finally, we're getting to use sklearn! You will train a logistic regression classifier to distinguish between two varieties of raisins, Kecimen and Besni, based on their size and shape. The raisins were photographed and features describing their size and shape were extracted from the images. The original dataset is available [here](https://www.kaggle.com/datasets/muratkokludataset/raisin-dataset). Take a look at the dataset. `Class` indicates raisin variety with True for Kecimen and False for Besni. All the other columns are size and shape features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d2651",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-53bb9a60f63447bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We will load the dataset for you\n",
    "raisins = pd.read_csv('data/raisins_dataset.csv')\n",
    "raisins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992fad2c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5280f518dc4e9b3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Implement a function that will train a sklearn logistic regression model on the `raisins` dataset. It should return the classifier instance, the probabilities for the raisins to be of Kecimen variety, and the coefficients of the model including the intercept.\n",
    "\n",
    "- use all available features to train the model\n",
    "- use `Class` as the target\n",
    "- standardize the features\n",
    "- fit a logistic regression for a maximum of 100 iterations and random state = 100 (look in the API reference for the necessary parameters)\n",
    "\n",
    "The input of the function is the `raisins` dataset. The output is the classifier, an array of probabilities, an array of model coefficients, and the model intercept. Notice that the target is encoded as True/False - sklearn will understand this. Make sure to return the probabilities of the positive class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4a5c71",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3366fd83ee5c006d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def train_model_sklearn(dataset):\n",
    "    '''\n",
    "    Fits logistic regression to the raisins dataset\n",
    "    and returns the classifier instance, the probabilities, the model coefficients and the intercept.\n",
    "    \n",
    "    Args:\n",
    "        dataset(pd.DataFrame): training dataset\n",
    "    \n",
    "    Returns:\n",
    "        clf: the classifier\n",
    "        probas (np.array): Array of floats with the probability \n",
    "                           of each raisin being the Kecimen variety\n",
    "        coefficients (np.array): coefficients of the trained logistic regression.\n",
    "        intercept (np.array): intercept of the trained logistic regression          \n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return clf, probas, coefficients, intercept\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b36c05",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a1336550d738ac46",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "lr, probas, coef, intercept = train_model_sklearn(raisins)\n",
    "\n",
    "assert str(lr)=='LogisticRegression(random_state=100)',\"Did you use the correct classifier?\"\n",
    "\n",
    "# Testing Probas\n",
    "np.testing.assert_almost_equal(round(probas.max()), 1), \"Something is wrong with your probabilities.\"\n",
    "np.testing.assert_almost_equal(round(probas.min()), 0), \"Something is wrong with your probabilities.\"\n",
    "np.testing.assert_almost_equal(round(probas.mean(),3), 0.500, 2), \"Something is wrong with your probabilities.\"\n",
    "np.testing.assert_almost_equal(round(probas.std(),5), 0.36992, 3), \"Something is wrong with your probabilities.\"\n",
    "np.testing.assert_almost_equal(round(probas.sum())*0.001, 0.450, 3), \"Something is wrong with your probabilities.\"\n",
    "\n",
    "# Testing Coefs\n",
    "assert coef.shape==(1,7), 'Wrong number of coefficients. Did you select the features correctly?'\n",
    "np.testing.assert_almost_equal(round(coef.max(),3), 0.733, 2), \"Something is wrong with your model coefficients.\"\n",
    "np.testing.assert_almost_equal(round(coef.min(),3), -2.333, 2), \"Something is wrong with your model coefficients.\"\n",
    "np.testing.assert_almost_equal(round(coef.mean(),4), -0.4395, 3), \"Something is wrong with your model coefficients.\"\n",
    "np.testing.assert_almost_equal(round(coef.var(),4), 0.7687, 3), \"Something is wrong with your model coefficients.\"\n",
    "np.testing.assert_almost_equal(round(coef.sum(),4), -3.0768, 3), \"Something is wrong with your model coefficients.\"\n",
    "\n",
    "assert hashlib.sha256(json.dumps(str(round(intercept[0],3))).encode()).hexdigest()=='ad5db3ccd28d807e79ebc49fcb89236070829c4145ec1ff8db72b06b06bcb350',\"Something is wrong with your intercept\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb792bd7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cff01023af490aee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 3.2: Decision boundary\n",
    "\n",
    "In general, the decision boundary in binary logistic regression is a hyperplane of dimension n -1 in the feature space, with n being the number of features. You can imagine this in 3D: it's like a cloud of observations cut with a decision boundary knife. Recall that you can derive the equation for this hyperplane from the logistic regression formula.\n",
    "\n",
    "For the classification model from exercise 3.1, calculate the value of the feature `Perimeter` on the decision boundary given the values of the other six features. Return it as a float of the same name. The values of the other six features are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98fc7a5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8d79cc6705b265b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# the features were scaled that's why the negative numbers\n",
    "Area = 1.94\n",
    "MajorAxisLength = 2.31\n",
    "MinorAxisLength = 0.92\n",
    "Eccentricity = -0.73\n",
    "ConvexArea=3.45\n",
    "Extent=-2.78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba482dc",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-681997214b36903b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb90231",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a89ab0677d624611",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(Perimeter,float), 'Perimeter should be a float, not an array'\n",
    "assert hashlib.sha256(json.dumps(str(round(Perimeter,1))).encode()).hexdigest()=='81cb0c0ea658f6d9b2de914a4cc2b72fb79b1f3453e18c843ccb64e6bc7b4aa6',\"Not correct, try again.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ed3738",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9fe6e671c58ff28f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 3.3: Logistic regression with less features\n",
    "\n",
    "Train another logistic regression for the `raisins` dataset, but use only two features, `MinorAxisLength` and `Perimeter`. As before, use max 100 iterations and set random_state to 100. Standardize the features.\n",
    "\n",
    "The input of the function is the `raisins` dataset. The output is the classifier, an array of probabilities of the positive class, an array of model coefficients, and the model intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31c8bb6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b0ee0b40d40beb41",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model_sklearn_2_features(dataset):\n",
    "    '''\n",
    "    Fits logistic regression to selected features of the raisins dataset\n",
    "    and returns the classifier, the probabilities, the model coefficients and the intercept.\n",
    "    Uses the features MinorAxisLength and Perimeter.\n",
    "    \n",
    "    Args:\n",
    "        dataset(pd.DataFrame): training dataset\n",
    "    \n",
    "    Returns:\n",
    "        clf: the classifier\n",
    "        probas (np.array): Array of floats with the probability \n",
    "                           of each raisin being the Kecimen variety\n",
    "        coefficients (np.array): coefficients of the trained logistic regression.\n",
    "        intercept (np.array): intercept of the trained logistic regression          \n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return clf, probas, coefficients, intercept\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416cc4c4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-bc37d3a63cd613c3",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "lr2, probas2, coef2, intercept2 = train_model_sklearn_2_features(raisins)\n",
    "\n",
    "assert str(lr2)=='LogisticRegression(random_state=100)',\"Did you use the correct classifier?\"\n",
    "\n",
    "# Testing Probas\n",
    "np.testing.assert_almost_equal(round(probas2.max()), 1), \"Something is wrong with your probabilities.\"\n",
    "np.testing.assert_almost_equal(round(probas2.min()), 0), \"Something is wrong with your probabilities.\"\n",
    "np.testing.assert_almost_equal(round(probas2.mean(),1), 0.5, 1), \"Something is wrong with your probabilities.\"\n",
    "np.testing.assert_almost_equal(round(probas2.std(),5), 0.36578, 3), \"Something is wrong with your probabilities.\"\n",
    "np.testing.assert_almost_equal(round(probas2.sum())*0.001, 0.450, 3), \"Something is wrong with your probabilities.\"\n",
    "\n",
    "# Testing Coefs\n",
    "assert coef2.shape==(1,2), 'Wrong number of coefficients. Did you select the features correctly?'\n",
    "np.testing.assert_almost_equal(round(coef2.mean(),3), -1.465, 2), \"Something is wrong with your model coefficients.\"\n",
    "np.testing.assert_almost_equal(round(coef2.var(),5), 5.98857, 3), \"Something is wrong with your model coefficients.\"\n",
    "np.testing.assert_almost_equal(round(coef2.sum(),3), -2.931, 2), \"Something is wrong with your model coefficients.\"\n",
    "\n",
    "assert hashlib.sha256(json.dumps(str(round(intercept2[0],2))).encode()).hexdigest()=='90248082ff854cb0699ef9c82c9514d456a536e4092bb1fd69e72e446dfc8cbd',\"Something is wrong with your intercept\"\n",
    "\n",
    "correct1,correct2=utils.compare_classifiers(lr,lr2,raisins.drop(columns=['Class']),\n",
    "                          raisins[['MinorAxisLength','Perimeter']],raisins.Class)\n",
    "print(\"The 7-feature model classified %d out of %d raisins correctly.\" % (correct1,raisins.shape[0]))\n",
    "print(\"The 2-feature model classified %d out of %d raisins correctly.\" % (correct2,raisins.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba9171c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f71c67d5bdf71154",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see, both models performed similarly well. It is because most of the features have little influence on the outcome. The most important feature is `Perimeter`. You can see it on the size of the corresponding model coefficient (uncomment and run the cell below). Selection of features and their importance for the model predictions will be discussed in SLU14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d936a9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b967385ebe96ca36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# uncomment this cell to see the coefficients\n",
    "# The higher is the absolute value of the coefficient, the more it influences the model predictions.\n",
    "# The order of the coefficients is the same as the order of features input into the model.\n",
    "#print('7-features model coefficients')\n",
    "#print(coef)\n",
    "#print('2-features model coefficients')\n",
    "#print(coef2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fe7dfa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f5dabdeb470baeeb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here is a plot of the datapoints and the decision boundary for the 2-feature model (with scaled features). We can't plot the result for the 7-feature model, the boundary cannot be projected into 2d space. Think about why!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b299e4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8a7871eedef5f834",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "utils.plot_exercise_boundary(raisins,['MinorAxisLength','Perimeter'],raisins.Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af0d5a8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-abe7912f5e7efd7b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Congratulations, you've learned to train your first classifier! But how good is your model at predicting the \n",
    "outcome? You will learn how to evaluate model performance using metrics in the next SLU!\n",
    "\n",
    "We have one more optional ungraded exercise below if you'd like to practice more. It is the same as exercise 3, just with another dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ade8bdf",
   "metadata": {},
   "source": [
    "<img src=\"https://imgs.xkcd.com/comics/machine_learning.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735e9c7c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6cc909f84c3ea96a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 4.1 - optional, ungraded\n",
    "\n",
    "The dataset for this exercise is the dependency of cannabis use on personality measures - neuroticism, extraversion, openness to experience, agreeableness, conscientiousness, impulsivity, and sensation seeking. It is a subset of [this dataset](https://www.kaggle.com/datasets/obeykhadija/drug-consumptions-uci)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17e7732",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-36a9326254d1dcac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "cannabis = pd.read_csv('data/cannabis_consumption.csv')\n",
    "cannabis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02fbc63",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7d7287ce58c90cb4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Implement a function that will train a sklearn logistic regression model on the `cannabis` dataset. It should return the classifier instance, the probabilities of cannabis use, and the coefficients of the model including the intercept.\n",
    "\n",
    "- use only the numerical features to train the model (Nscore, Escore, Oscore, Ascore, Cscore, Impulsive, SS)\n",
    "- use `Cannabis` as the target which is True for use in the past year\n",
    "- standardize the features\n",
    "- fit a logistic regression for a maximum of 100 iterations and random state = 100\n",
    "\n",
    "The input of the function is the `cannabis` dataset. The output is the classifier, an array of probabilities, an array of model coefficients, and the model intercept. Make sure to return the probabilities of the positive class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cbf506",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b87f36c9dd26956f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model_sklearn_cannabis(dataset):\n",
    "    '''\n",
    "    Fits logistic regression to the cannabis dataset\n",
    "    using the numerical features Nscore, Escore, Oscore, Ascore, Cscore, Impulsive, SS\n",
    "    and returns the classifier instance, the probabilities, the model coefficients and the intercept.\n",
    "    \n",
    "    Args:\n",
    "        dataset(pd.DataFrame): training dataset\n",
    "    \n",
    "    Returns:\n",
    "        clf: the classifier\n",
    "        probas (np.array): array of floats with the probability \n",
    "                           of cannabis use in the past year\n",
    "        coefficients (np.array): coefficients of the trained logistic regression.\n",
    "        intercept (np.array): intercept of the trained logistic regression          \n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return clf, probas, coefficients, intercept\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e757bc0c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-111f4f16bc9435ff",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "lr_can, probas_can, coef_can, intercept_can = train_model_sklearn_cannabis(cannabis)\n",
    "\n",
    "assert str(lr_can)=='LogisticRegression(random_state=100)',\"Did you use the correct classifier?\"\n",
    "\n",
    "# Testing Probasa\n",
    "np.testing.assert_almost_equal(round(probas_can.max()), 1), \"Something is wrong with your probabilities.\"\n",
    "np.testing.assert_almost_equal(round(probas_can.min(),5), 0.00993,3), \"Something is wrong with your probabilities.\"\n",
    "np.testing.assert_almost_equal(round(probas_can.mean(),5), 0.46975, 3), \"Something is wrong with your probabilities.\"\n",
    "np.testing.assert_almost_equal(round(probas_can.std(),5), 0.27543, 3), \"Something is wrong with your probabilities.\"\n",
    "np.testing.assert_almost_equal(round(probas_can.sum())*0.001, 0.885, 3), \"Something is wrong with your probabilities.\"\n",
    "\n",
    "# Testing Coefs\n",
    "assert coef_can.shape==(1,7), 'Wrong number of coefficients. Did you select the features correctly?'\n",
    "np.testing.assert_almost_equal(round(coef_can.max(),3), 0.477, 2), \"Something is wrong with your model coefficients.\"\n",
    "np.testing.assert_almost_equal(round(coef_can.min(),5), -0.90780, 3), \"Something is wrong with your model coefficients.\"\n",
    "np.testing.assert_almost_equal(round(coef_can.mean(),5), -0.08774, 3), \"Something is wrong with your model coefficients.\"\n",
    "np.testing.assert_almost_equal(round(coef_can.var(),3), 0.251, 2), \"Something is wrong with your model coefficients.\"\n",
    "np.testing.assert_almost_equal(round(coef_can.sum(),3), -0.614, 2), \"Something is wrong with your model coefficients.\"\n",
    "\n",
    "assert hashlib.sha256(json.dumps(str(round(intercept_can[0],3))).encode()).hexdigest()=='a3251684a60052de243146a079bd8615c090c8ba312602a6f99bad546bec8ba9',\"Something is wrong with your intercept\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3f14c0",
   "metadata": {},
   "source": [
    "### Exercise 4.2 - optional ungraded\n",
    "\n",
    "Now train another logistic regression model on the same target using only the Oscore and SS features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da499de2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-62f55de589e1ccc2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model_sklearn_2_features_cannabis(dataset):\n",
    "    '''\n",
    "    Fits logistic regression to selected features of the cannabis dataset\n",
    "    and returns the classifier, the probabilities, the model coefficients and the intercept.\n",
    "    Uses the features Oscore and SS.\n",
    "    \n",
    "    Args:\n",
    "        dataset(pd.DataFrame): training dataset\n",
    "    \n",
    "    Returns:\n",
    "        clf: the classifier\n",
    "        probas (np.array): array of floats with the probability \n",
    "                           of cannabis use in the past year\n",
    "        coefficients (np.array): coefficients of the trained logistic regression.\n",
    "        intercept (np.array): intercept of the trained logistic regression          \n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return clf, probas, coefficients, intercept\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2327f40c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-356ec1b06e4cef66",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "lr_can2, probas_can2, coef_can2, intercept_can2 = train_model_sklearn_2_features_cannabis(cannabis)\n",
    "\n",
    "assert str(lr_can2)=='LogisticRegression(random_state=100)',\"Did you use the correct classifier?\"\n",
    "\n",
    "# Testing Probas\n",
    "np.testing.assert_almost_equal(round(probas_can2.max(),3), 0.975, 2), \"Something is wrong with your probabilities.\"\n",
    "np.testing.assert_almost_equal(round(probas_can2.min(),4), 0.0209, 3), \"Something is wrong with your probabilities.\"\n",
    "np.testing.assert_almost_equal(round(probas_can2.mean(),4), 0.4697, 2), \"Something is wrong with your probabilities.\"\n",
    "np.testing.assert_almost_equal(round(probas_can2.std(),4), 0.2508, 3), \"Something is wrong with your probabilities.\"\n",
    "np.testing.assert_almost_equal(round(probas_can2.sum())*0.001, 0.885, 3), \"Something is wrong with your probabilities.\"\n",
    "\n",
    "# Testing Coefs\n",
    "assert coef_can2.shape==(1,2), 'Wrong number of coefficients. Did you select the features correctly?'\n",
    "np.testing.assert_almost_equal(round(coef_can2.mean(),3), -0.773, 2), \"Something is wrong with your model coefficients.\"\n",
    "np.testing.assert_almost_equal(round(coef_can2.var(),5), 0.01275, 3), \"Something is wrong with your model coefficients.\"\n",
    "np.testing.assert_almost_equal(round(coef_can2.sum(),3), -1.547, 2), \"Something is wrong with your model coefficients.\"\n",
    "\n",
    "assert hashlib.sha256(json.dumps(str(round(intercept_can2[0],2))).encode()).hexdigest()=='2c166a03f5d797741a30f163766b3a052ec21f52c10c6fefbd5dc11912632748',\"Something is wrong with your intercept\"\n",
    "\n",
    "correct1,correct2=utils.compare_classifiers(lr_can,lr_can2,cannabis.select_dtypes(include='number').drop(columns=['ID']),\n",
    "                          cannabis[['Oscore','SS']],cannabis.Cannabis)\n",
    "print(\"The 7-feature model classified %d out of %d subjects correctly.\" % (correct1,cannabis.shape[0]))\n",
    "print(\"The 2-feature model classified %d out of %d subjects correctly.\" % (correct2,cannabis.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5b6907",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9b498edb7dc45897",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Again, both models performed similarly well because the features Oscore and SS are the most important ones. Uncomment the cell below to see the model coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ddfcd5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-daeed9609c54dea9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# uncomment this cell to see the coefficients\n",
    "# The higher is the absolute value of the coefficient, the more it influences the model predictions.\n",
    "# The order of the coefficients is the same as the order of features input into the model.\n",
    "#print('7-features model coefficients')\n",
    "#print(coef_can)\n",
    "#print('2-features model coefficients')\n",
    "#print(coef_can2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f9a48a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-695dce325afd368b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here is a plot of the datapoints and the decision boundary for the 2-feature model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee5b118",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e8978e4b53d7868f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "utils.plot_exercise_boundary(cannabis,['Oscore','SS'],cannabis.Cannabis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b380c4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66d8a2e58850b823",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The original Kaggle dataset has data on several drugs, not just cannabis, so you can practice even more. :) Just make sure that you choose a balanced target."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
