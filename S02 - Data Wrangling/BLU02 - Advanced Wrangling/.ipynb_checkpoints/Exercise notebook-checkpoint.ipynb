{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b66f2958",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3bf24e7ba2c45b1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# BLU02 - Exercises Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd525da6",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bebba7f87f4f151b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import hashlib # for grading\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b2b87c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a23783765364606a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1 Read the Programs data (graded)\n",
    "\n",
    "In this first exercise, we aim to create a single dataframe, combining all programs from all seasons.\n",
    "\n",
    "With a caveat though: **we want to include seasons from the year 1950 onwards**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e539f29e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e16abeb47fc4ea37",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def read_season(folder_path, file_name):\n",
    "    path = os.path.join(folder_path, file_name)\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def read_programs(folder_path):\n",
    "    files = os.listdir(folder_path)\n",
    "    # Create a list with the name of all files containing programs from\n",
    "    # 1950 inclusive and onwards (just the filename, no complete path.)\n",
    "    # files_from_1950: List[str] = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    files_from_1950 = [f for f in files if int(f.split('-')[0]) >= 1950]\n",
    "    ### END SOLUTION \n",
    "    # Create a list with the dataframes\n",
    "    # seasons: List[pd.DataFrame] = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    seasons = [read_season(folder_path, f) for f in files_from_1950 if '.csv' in f]\n",
    "    ### END SOLUTION\n",
    "    # Use pd.concat to create a single dataframe.\n",
    "    # programs: pd.DataFrame = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    programs = pd.concat(seasons, axis=0, ignore_index=True)\n",
    "    ### END SOLUTION\n",
    "    # Drop the column GUID.\n",
    "    # programs = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    programs = programs.drop(columns='GUID')\n",
    "    ### END SOLUTION\n",
    "    ## Remove Duplicated lines.\n",
    "    ### BEGIN SOLUTION\n",
    "    # programs = ...\n",
    "    programs = programs.drop_duplicates()\n",
    "    ### END SOLUTION\n",
    "    # Set the index to be the column ProgramID\n",
    "    ### BEGIN SOLUTION\n",
    "    programs = programs.set_index('ProgramID')\n",
    "    ### END SOLUTION\n",
    "    return programs\n",
    "\n",
    "programs = read_programs(os.path.join('data', 'programs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d5b3ffb",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2656708135a5a5e4",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert programs['Season'].min() == '1950-51'\n",
    "assert programs['Season'].max() == '2016-17'\n",
    "assert programs.index.name == 'ProgramID'\n",
    "assert programs.shape == (7341, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31920670",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3383047fa18f453e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2 Read the Concerts data (graded)\n",
    "\n",
    "Although we list all transformations step-by-step for the sake of clarity, we expect you to use method chaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0f86c9d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e228c2d82463c6b4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def make_concerts(file_path): \n",
    "    # Read concerts data and drop the GUID column.\n",
    "    # concerts: pd.DataFrame = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    concerts = pd.read_csv(file_path)\n",
    "    concerts = concerts.drop(columns=['GUID'])\n",
    "    ### END SOLUTION\n",
    "    # Remember to_datetime? We need it here. We need to parse the columns Date and \n",
    "    # Time. Use pd.to_datetime(...).dt.date for the Date. \n",
    "    # then use the same logic to create the column Hour and Minute from Time column.\n",
    "    ### BEGIN SOLUTION\n",
    "    concerts = concerts.assign(\n",
    "        Date=pd.to_datetime(concerts['Date']).dt.date,\n",
    "        Hour=pd.to_datetime(\n",
    "            concerts['Time']).dt.hour,\n",
    "        Minute=pd.to_datetime(\n",
    "            concerts['Time']).dt.minute,\n",
    "    )\n",
    "    ### END SOLUTION\n",
    "    ## Remove Duplicated lines.\n",
    "    ### BEGIN SOLUTION\n",
    "    concerts = concerts.drop_duplicates()\n",
    "    ### END SOLUTION\n",
    "    ## Remove all lines with empty Time column. Then also drop the Time column.\n",
    "    ### BEGIN SOLUTION\n",
    "    concerts = concerts.dropna(subset=[\"Time\"])\n",
    "    concerts = concerts.drop(\"Time\", axis = 1)\n",
    "    ### END SOLUTION    \n",
    "    \n",
    "    return concerts\n",
    "\n",
    "concerts = make_concerts(os.path.join('data','concerts.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52e0b5f0",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3d21007e725ab889",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert concerts.shape == (20812, 8)\n",
    "assert concerts.Date.min() == datetime.date(1842, 12, 7)\n",
    "assert concerts.Date.max() == datetime.date(2017, 7, 7)\n",
    "assert concerts.Date.max() == datetime.date(2017, 7, 7)\n",
    "assert concerts['Hour'].mode().values[0] == 20\n",
    "assert concerts['Minute'].mode().values[0] == 0\n",
    "assert list(concerts.iloc[1537][['Hour', 'Minute']].values) == [20,30]\n",
    "assert list(concerts.iloc[1201][['Hour', 'Minute']].values) == [20,15]\n",
    "assert set(concerts.columns) == set([\n",
    "    'ProgramID', 'ConcertID', 'EventType', 'Location', 'Venue', 'Date', 'Hour', 'Minute'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d6c904",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2ae195e6dd100fc5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3 Combine Programs and Concerts data (graded)\n",
    "\n",
    "Let's combine both dataframes into a single dataset, using an inner join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61a56101",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a65f1464b4525a8b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Remember that you want to join on the index of one of the dataframes.\n",
    "# Join only the concerts with valid ProgramIDs\n",
    "# nyp = ...\n",
    "### BEGIN SOLUTION\n",
    "nyp = concerts.join(programs, on='ProgramID', how='inner')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26483891",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ac9aef3d5251e36c",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert nyp.shape == (12943, 10)\n",
    "assert set(nyp.columns) == set([\n",
    "    'ProgramID', 'ConcertID', 'EventType', 'Location', 'Venue',\n",
    "    'Date', 'Hour', 'Minute', 'Orchestra', 'Season'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71271ecf",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1fd4cd11d5139889",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4 Read Works and Soloists data (graded)\n",
    "\n",
    "We will read the two remaining pieces of data. \n",
    "\n",
    "Again, albeit the step-by-step description, we encourage you to use method chaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb533fb6",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-503e208490ff38e0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def read_works(file_path):\n",
    "    # Read the works data.\n",
    "    # works: pd.DataFrame = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    works = pd.read_csv(file_path)\n",
    "    ### END SOLUTION\n",
    "    # Remove the Intervals (attention to the values in the isInterval column).\n",
    "    # works: pd.DataFrame = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    works = works[~works.isInterval]\n",
    "    ### END SOLUTION\n",
    "    # Select the columns ProgramID, WorkID, ComposerName, WorkTitle, Movement and ConductorName.\n",
    "    ### BEGIN SOLUTION\n",
    "    columns = [\n",
    "        'ProgramID','WorkID', \n",
    "        'ComposerName', 'WorkTitle', 'Movement', \n",
    "        'ConductorName'\n",
    "    ]\n",
    "    works = works.loc[:, columns]\n",
    "    ### END SOLUTION\n",
    "    ## Remove Duplicated lines.\n",
    "    ### BEGIN SOLUTION\n",
    "    # works: pd.DataFrame = ...\n",
    "    works = works.drop_duplicates()\n",
    "    ### END SOLUTION\n",
    "    ## Remove all lines with empty Movement column.\n",
    "    ### BEGIN SOLUTION\n",
    "    # works: pd.DataFrame = ...\n",
    "    works = works.dropna(subset=[\"Movement\"])\n",
    "    ### END SOLUTION    \n",
    "    \n",
    "    return works\n",
    "\n",
    "\n",
    "def read_soloists(file_path):\n",
    "    # Read the soloists data and drop GUID and MovementID Columns\n",
    "    ### BEGIN SOLUTION\n",
    "    soloists = pd.read_csv(file_path)\n",
    "    soloists = soloists.drop(columns=['GUID', 'MovementID'])\n",
    "    ### END SOLUTION\n",
    "    ## Remove Duplicated lines.\n",
    "    ### BEGIN SOLUTION\n",
    "    # soloists: pd.DataFrame = ...\n",
    "    soloists = soloists.drop_duplicates()\n",
    "    ### END SOLUTION\n",
    "    return soloists\n",
    "\n",
    "\n",
    "works = read_works('data/works.csv')\n",
    "soloists = read_soloists('data/soloists.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11f60f07",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b8389314995f18ea",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert works.shape == (24320, 6)\n",
    "assert set(works.columns) == set([\n",
    "    'ProgramID', 'WorkID', 'ComposerName', 'WorkTitle', 'Movement', 'ConductorName'\n",
    "])\n",
    "\n",
    "assert soloists.shape == (50292, 5)\n",
    "assert set(soloists.columns) == set([\n",
    "   'ProgramID', 'WorkID', 'SoloistName', 'SoloistInstrument', 'SoloistRole'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839081fe",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c16e4e26e68cd019",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 5 Combine Works and Soloists (graded)\n",
    "\n",
    "Like we did for Programs and Concerts, now we combine Works and Soloists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfae5297",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-08d9a086cc5646cf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Combine both dataframes, again using an inner type of join. An work is identified by the pair\n",
    "# ProgramId, WorkID\n",
    "# works_and_soloists : pd.DataFrame = ....\n",
    "### BEGIN SOLUTION\n",
    "works_and_soloists = pd.merge(works, soloists, on=['WorkID', 'ProgramID'])\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7184d4cd",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4d9f103dfffd311b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert works_and_soloists.shape == (23578, 9)\n",
    "assert set(works_and_soloists.columns) == set(\n",
    "    [\n",
    "        'ProgramID', 'WorkID', 'ComposerName', 'WorkTitle', 'Movement',\n",
    "        'ConductorName', 'SoloistName', 'SoloistInstrument', 'SoloistRole'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cddd282",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ab79800d6e447f1e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 6 Combine everything (graded)\n",
    "\n",
    "The final goal here is to create a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a4eee72",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ce1f05022e8cd63a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Combine works_and_soloists and nyp into a single dataframe.\n",
    "# You need to figure out the common column shared between the two dataframes\n",
    "# nyp_merged = ...\n",
    "### BEGIN SOLUTION\n",
    "nyp_merged = pd.merge(nyp, works_and_soloists, on=['ProgramID'])\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55c7dc60",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ce29fa3aec1c244e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert nyp_merged.shape == (27725, 18)\n",
    "assert set(nyp_merged.columns) == set(\n",
    "    [\n",
    "       'ProgramID', 'ConcertID', 'EventType', 'Location', 'Venue', 'Date',\n",
    "       'Hour', 'Minute', 'Orchestra', 'Season', 'WorkID', 'ComposerName', 'WorkTitle',\n",
    "       'Movement', 'ConductorName', 'SoloistName', 'SoloistInstrument',\n",
    "       'SoloistRole'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73c7125",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4407d9518b0d6e2e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 7 Final transformations (graded)\n",
    "\n",
    "Now, we perform the train-test split.\n",
    "\n",
    "We also perform some final transformations on both datasets:\n",
    "\n",
    "* Include some date features: Year, Month, Day and Weekday\n",
    "* Create a new feature, ComposerLastName from ComposerName column. \n",
    "* Filter out rows with a location that appears less than 10 times in the DataFrame.\n",
    "* Drop ProgramID, ConcertID, WorkID, Date and Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d5094ff",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1c1ab0d912e615ca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def append_date_features(df):\n",
    "    df = df.copy()\n",
    "    # Use the chain method to create multiple datatime columns\n",
    "    ### BEGIN SOLUTION\n",
    "    df = df.assign(\n",
    "        Year=pd.to_datetime(df['Date']).dt.year,\n",
    "        Month=pd.to_datetime(df['Date']).dt.month,\n",
    "        Day=pd.to_datetime(df['Date']).dt.day,\n",
    "        Weekday=pd.to_datetime(df['Date']).dt.weekday\n",
    "    )\n",
    "    ### END SOLUTION\n",
    "    return df\n",
    "\n",
    "def append_composer_last_name(df):\n",
    "    ### BEGIN SOLUTION\n",
    "    df['ComposerLastName'] = df.ComposerName.map(lambda x: x.split(',')[0])\n",
    "    ### END SOLUTION\n",
    "    return df\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # You should follow these exact steps:\n",
    "    #   1 - Include some date features: Year, Month, Hour, Minute, Day and Weekday\n",
    "    #   2 - Create a new feature, ComposerLastName from ComposerName column. \n",
    "    #   3 - Filter out rows that have a location that appear is less than 10 times in the DataFrame.\n",
    "    #   4 - Drop ProgramID, ConcertID, WorkID, Season, Date, Time\n",
    "    #   \n",
    "    ### BEGIN SOLUTION\n",
    "    df = df.copy()\n",
    "    df = df.pipe(\n",
    "                append_date_features\n",
    "            ).pipe(\n",
    "                append_composer_last_name\n",
    "            ).groupby(\n",
    "                'Location'\n",
    "            ).filter(\n",
    "                lambda x: x.shape[0] >= 10\n",
    "            ).drop(\n",
    "                columns=['ProgramID', 'ConcertID', 'WorkID', 'Season', 'Date']\n",
    "            )\n",
    "    ### END SOLUTION\n",
    "    return df\n",
    "\n",
    "\n",
    "nyp_preprocessed = preprocess_data(nyp_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee21e1d5",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9d9c75c48e4eaf63",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert nyp_preprocessed.shape == (27571, 18)\n",
    "assert set(nyp_preprocessed.columns) == {\n",
    "       'EventType', 'Location', 'Venue', 'Orchestra',\n",
    "       'ComposerName', 'WorkTitle', 'Movement', 'ConductorName', 'SoloistName',\n",
    "       'SoloistInstrument', 'SoloistRole', 'Year', 'Month', 'Day', 'Hour',\n",
    "       'Minute', 'Weekday', 'ComposerLastName'\n",
    "}\n",
    "assert nyp_preprocessed.groupby('Location').size().min() == 10\n",
    "assert nyp_preprocessed.ComposerLastName.value_counts().loc['Mozart'] == 512\n",
    "assert nyp_preprocessed.ComposerLastName.value_counts().loc['Gershwin'] == 1673\n",
    "assert nyp_preprocessed.ComposerLastName.nunique() == 236"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4918d1",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4557c57da6142038",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# The house prices dataset\n",
    "\n",
    "A dataset containing several characteristics of several houses and their selling price \n",
    "\n",
    "* LotFrontage: Linear feet of street connected to property\n",
    "* LotArea: Lot size in square feet\n",
    "* OverallQual: Rates the overall material and finish of the house\n",
    "       10  Very Excellent\n",
    "       9\tExcellent\n",
    "       8\tVery Good\n",
    "       7\tGood\n",
    "       6\tAbove Average\n",
    "       5\tAverage\n",
    "       4\tBelow Average\n",
    "       3\tFair\n",
    "       2\tPoor\n",
    "       1\tVery Poor\n",
    "* OverallCond: Rates the overall condition of the house\n",
    "\n",
    "       10\tVery Excellent\n",
    "       9\tExcellent\n",
    "       8\tVery Good\n",
    "       7\tGood\n",
    "       6\tAbove Average\t\n",
    "       5\tAverage\n",
    "       4\tBelow Average\t\n",
    "       3\tFair\n",
    "       2\tPoor\n",
    "       1\tVery Poor\n",
    "* MasVnrArea: Masonry veneer area in square feet\n",
    "* BsmtFinSF1: Type 1 finished square feet\n",
    "* BsmtUnfSF: Unfinished square feet of basement area\n",
    "* TotalBsmtSF: Total square feet of basement area\n",
    "* 1stFlrSF: First Floor square feet\n",
    "* 2ndFlrSF: Second floor square feet\n",
    "* LowQualFinSF: Low quality finished square feet (all floors)\n",
    "* GrLivArea: Above grade (ground) living area square feet\n",
    "* BsmtFullBath: Basement full bathrooms\n",
    "* BsmtHalfBath: Basement half bathrooms\n",
    "* FullBath: Full bathrooms above grade\n",
    "* HalfBath: Half baths above grade\n",
    "* BedroomAbvGr: Bedrooms above grade (does NOT include basement bedrooms)\n",
    "* KitchenAbvGr: Kitchens above grade\n",
    "* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n",
    "* Fireplaces: Number of fireplaces\n",
    "* GarageCars: Size of garage in car capacity\n",
    "* GarageArea: Size of garage in square feet\n",
    "* WoodDeckSF: Wood deck area in square feet\n",
    "* OpenPorchSF: Open porch area in square feet\n",
    "* EnclosedPorch: Enclosed porch area in square feet\n",
    "* 3SsnPorch: Three season porch area in square feet\n",
    "* ScreenPorch: Screen porch area in square feet\n",
    "* PoolArea: Pool area in square feet\n",
    "* MiscVal: $Value of miscellaneous feature \n",
    "* SellingDate: Date when the house was sold\n",
    "* BuildingDate: Date when the house was built\n",
    "* RemodAddDate: Remodel date (same as construction date if no remodeling or additions)\n",
    "* SalePrice: The house price at the selling date (our target variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd8c63",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2f274d778f5887e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's read the csv and create our train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "049fd662",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7582e93ac7adb85f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def house_price_dataset():\n",
    "    return pd.read_csv(\n",
    "    'data/housePrices.csv', \n",
    "        parse_dates=[\n",
    "            'SellingDate',\n",
    "            'BuildingDate',\n",
    "            'RemodAddDate'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "dataset = house_price_dataset()\n",
    "dataset_train, dataset_test = train_test_split(dataset, random_state=0)\n",
    "X_train = dataset_train.drop(columns='SalePrice')\n",
    "y_train = dataset_train.SalePrice\n",
    "X_test = dataset_test.drop(columns='SalePrice')\n",
    "y_test = dataset_test.SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fa1fe4",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-425695b0c9d45ae9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 8 Build a DateTransformer transformer (graded)\n",
    "\n",
    "There's a simple transformer that can be useful, from times to times, when modeling.\n",
    "\n",
    "What we want is to build a transformer that transforms dates into timedeltas.\n",
    "\n",
    "Usually when you have features that are Dates you compute a time delta between the feature and a given refence date.\n",
    "\n",
    "e.g Imagine that your clients have a loyalty period that ends at a given date. When your model is doing some predictions, one of the features that you can use is the number of days until the end of the loyalty period. i.e the date when the loyalty ends minus the date when your model is running. \n",
    "\n",
    "In the house prices dataset, the selling date will be the reference data, since we want to predict the house price at the selling date. For instance, two houses with the exact same features can vary in prices if the construction year is different. So we should input this information and feed into the model. Then we need to convert the other dates using our transformer\n",
    "\n",
    "Hint: Result should be integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43b78c45",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d9538038181ba578",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DateTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Implement the __init__ method.\n",
    "    # Our DateTransformer must be able to receive two parameters: \n",
    "    # datetime_cols: a list, that contains the datetime cols that should be converted\n",
    "    # ref_date_col - indicates the name of the column that should be used as reference date,\n",
    "    ### BEGIN SOLUTION\n",
    "    def __init__(self, datetime_cols, ref_date_col):\n",
    "        self.ref_date_col = ref_date_col\n",
    "        self.datetime_cols = datetime_cols\n",
    "    ### END SOLUTION\n",
    "        \n",
    "    # There's no need for a fit method in this case, it does nothing.\n",
    "    # We should be able to call fit without any explicit parameters.\n",
    "    # Meaning: we should be able to call transformer.fit().\n",
    "    ### BEGIN SOLUTION\n",
    "    def fit(self, X=None, y=None):\n",
    "        return self\n",
    "    ### END SOLUTION\n",
    "\n",
    "    # Transform should transform all datetime columns into the difference in days to the reference date.\n",
    "    # The reference date column should be dropped. \n",
    "    ### BEGIN SOLUTION\n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        for col in self.datetime_cols:\n",
    "            X_[col] = (X_[col] - X_[self.ref_date_col]).dt.days\n",
    "        return X_.drop(columns='SellingDate')\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bab1d26c",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b64b26753ecd8561",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train_transformed = DateTransformer(\n",
    "    datetime_cols=['BuildingDate', 'RemodAddDate'], \n",
    "    ref_date_col='SellingDate'\n",
    ").fit_transform(X_train)\n",
    "assert X_train_transformed.BuildingDate.min() == -49008\n",
    "assert X_train_transformed.BuildingDate.max() == -1\n",
    "assert 'SellingDate' not in X_train_transformed.columns\n",
    "assert X_train_transformed.dtypes.BuildingDate == np.dtype('int64')\n",
    "assert X_train_transformed.dtypes.RemodAddDate == np.dtype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dc7b15",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5989c2b51d38b449",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You might be wondering why we have to implement it as a Transformer Class, and not using functions.\n",
    "You'll understand the reason in the next section - so we can tie them all together in a `Pipeline`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714780de",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-46fe8c71f80d2717",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 9 Building the pipeline (graded)\n",
    "\n",
    "Finally, we want to use the two transformers together and run a linear regression on top.\n",
    "\n",
    "We want to Convert the dates to time deltas relative to the Selling Date.\n",
    "\n",
    "We want to scale all features to the same range, using `sklearn.preprocessing.StandardScaler()`.\n",
    "\n",
    "We want to estimate the SellingPrice using a Liner Regression.\n",
    "\n",
    "Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.\n",
    "\n",
    "In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.\n",
    "\n",
    "For instance, many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "697faf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>LowQualFinSF</th>\n",
       "      <th>...</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1051.000000</td>\n",
       "      <td>1051.000000</td>\n",
       "      <td>1051.000000</td>\n",
       "      <td>1051.000000</td>\n",
       "      <td>1051.000000</td>\n",
       "      <td>1051.000000</td>\n",
       "      <td>1051.000000</td>\n",
       "      <td>1051.000000</td>\n",
       "      <td>1051.000000</td>\n",
       "      <td>1051.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1051.000000</td>\n",
       "      <td>1051.000000</td>\n",
       "      <td>1051.000000</td>\n",
       "      <td>1051.000000</td>\n",
       "      <td>1051.000000</td>\n",
       "      <td>1051.000000</td>\n",
       "      <td>1051.000000</td>\n",
       "      <td>1051.000000</td>\n",
       "      <td>1051.000000</td>\n",
       "      <td>1051.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>56.516651</td>\n",
       "      <td>10165.033302</td>\n",
       "      <td>6.095147</td>\n",
       "      <td>5.594672</td>\n",
       "      <td>102.087536</td>\n",
       "      <td>564.330162</td>\n",
       "      <td>1046.601332</td>\n",
       "      <td>1159.809705</td>\n",
       "      <td>351.509039</td>\n",
       "      <td>6.424358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.627022</td>\n",
       "      <td>1.760228</td>\n",
       "      <td>470.736441</td>\n",
       "      <td>97.315890</td>\n",
       "      <td>44.084681</td>\n",
       "      <td>21.963844</td>\n",
       "      <td>3.058991</td>\n",
       "      <td>15.811608</td>\n",
       "      <td>2.394862</td>\n",
       "      <td>28.690771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>33.228422</td>\n",
       "      <td>6319.536187</td>\n",
       "      <td>1.365732</td>\n",
       "      <td>1.120939</td>\n",
       "      <td>179.723470</td>\n",
       "      <td>442.621653</td>\n",
       "      <td>418.210100</td>\n",
       "      <td>380.318077</td>\n",
       "      <td>437.579545</td>\n",
       "      <td>52.580304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.648656</td>\n",
       "      <td>0.744233</td>\n",
       "      <td>209.647682</td>\n",
       "      <td>124.988613</td>\n",
       "      <td>62.975199</td>\n",
       "      <td>60.397025</td>\n",
       "      <td>27.657847</td>\n",
       "      <td>56.470192</td>\n",
       "      <td>39.015517</td>\n",
       "      <td>190.231875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1491.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>7500.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>217.500000</td>\n",
       "      <td>792.500000</td>\n",
       "      <td>876.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>336.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>9505.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>463.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>1077.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>78.000000</td>\n",
       "      <td>11635.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>808.000000</td>\n",
       "      <td>1282.500000</td>\n",
       "      <td>1382.500000</td>\n",
       "      <td>736.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>576.000000</td>\n",
       "      <td>177.500000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>182.000000</td>\n",
       "      <td>115149.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>2336.000000</td>\n",
       "      <td>3200.000000</td>\n",
       "      <td>3228.000000</td>\n",
       "      <td>2065.000000</td>\n",
       "      <td>572.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1390.000000</td>\n",
       "      <td>728.000000</td>\n",
       "      <td>547.000000</td>\n",
       "      <td>386.000000</td>\n",
       "      <td>508.000000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>738.000000</td>\n",
       "      <td>3500.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       LotFrontage        LotArea  OverallQual  OverallCond   MasVnrArea  \\\n",
       "count  1051.000000    1051.000000  1051.000000  1051.000000  1051.000000   \n",
       "mean     56.516651   10165.033302     6.095147     5.594672   102.087536   \n",
       "std      33.228422    6319.536187     1.365732     1.120939   179.723470   \n",
       "min       0.000000    1491.000000     1.000000     2.000000     0.000000   \n",
       "25%      40.000000    7500.000000     5.000000     5.000000     0.000000   \n",
       "50%      61.000000    9505.000000     6.000000     5.000000     0.000000   \n",
       "75%      78.000000   11635.000000     7.000000     6.000000   161.000000   \n",
       "max     182.000000  115149.000000    10.000000     9.000000  1600.000000   \n",
       "\n",
       "         BsmtUnfSF  TotalBsmtSF     1stFlrSF     2ndFlrSF  LowQualFinSF  ...  \\\n",
       "count  1051.000000  1051.000000  1051.000000  1051.000000   1051.000000  ...   \n",
       "mean    564.330162  1046.601332  1159.809705   351.509039      6.424358  ...   \n",
       "std     442.621653   418.210100   380.318077   437.579545     52.580304  ...   \n",
       "min       0.000000     0.000000   334.000000     0.000000      0.000000  ...   \n",
       "25%     217.500000   792.500000   876.000000     0.000000      0.000000  ...   \n",
       "50%     463.000000   990.000000  1077.000000     0.000000      0.000000  ...   \n",
       "75%     808.000000  1282.500000  1382.500000   736.000000      0.000000  ...   \n",
       "max    2336.000000  3200.000000  3228.000000  2065.000000    572.000000  ...   \n",
       "\n",
       "        Fireplaces   GarageCars   GarageArea   WoodDeckSF  OpenPorchSF  \\\n",
       "count  1051.000000  1051.000000  1051.000000  1051.000000  1051.000000   \n",
       "mean      0.627022     1.760228   470.736441    97.315890    44.084681   \n",
       "std       0.648656     0.744233   209.647682   124.988613    62.975199   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     1.000000   336.000000     0.000000     0.000000   \n",
       "50%       1.000000     2.000000   477.000000     0.000000    24.000000   \n",
       "75%       1.000000     2.000000   576.000000   177.500000    64.500000   \n",
       "max       3.000000     4.000000  1390.000000   728.000000   547.000000   \n",
       "\n",
       "       EnclosedPorch    3SsnPorch  ScreenPorch     PoolArea      MiscVal  \n",
       "count    1051.000000  1051.000000  1051.000000  1051.000000  1051.000000  \n",
       "mean       21.963844     3.058991    15.811608     2.394862    28.690771  \n",
       "std        60.397025    27.657847    56.470192    39.015517   190.231875  \n",
       "min         0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%         0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "50%         0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "75%         0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "max       386.000000   508.000000   480.000000   738.000000  3500.000000  \n",
       "\n",
       "[8 rows x 28 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec0afe11",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6bc09de5e71383d3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 20737.060193147765\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline including:\n",
    "#   1 - 'date_converter', DateTransformer(['BuildingDate', 'RemodAddDate'], ref_date_col='SellingDate')\n",
    "#   2 - 'standard_scaler', StandardScaler() with the default parameters\n",
    "#   3 - 'model', LinearRegression\n",
    "### BEGIN SOLUTION\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            'date_converter', \n",
    "            DateTransformer(\n",
    "                ['BuildingDate', 'RemodAddDate'], \n",
    "                ref_date_col='SellingDate'\n",
    "            )\n",
    "        ),\n",
    "        (\n",
    "            'robust_scaler', \n",
    "            RobustScaler()\n",
    "        ),\n",
    "        (\n",
    "            'model', \n",
    "            LinearRegression()\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "### END SOLUTION\n",
    "\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print('MAE: {}'.format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01715551",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5d21dcdd34a13d24",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(pipeline) == Pipeline\n",
    "assert type(pipeline.named_steps['date_converter']) == DateTransformer\n",
    "assert type(pipeline.named_steps['robust_scaler']) == RobustScaler\n",
    "assert pipeline.named_steps['date_converter'].get_params()['ref_date_col'] == 'SellingDate'\n",
    "assert set(\n",
    "    pipeline.named_steps['date_converter'].get_params()['datetime_cols']\n",
    ") == {'BuildingDate', 'RemodAddDate'}\n",
    "assert type(pipeline.named_steps['model']) == LinearRegression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97367e2",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-154280375c499868",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 10. Access the cofficients from the pipeline (ungraded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0f0738",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9a913e812dcb37ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we would want to obtain the coefficients from the model to understand features with the most predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6404b9be",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-58083ef8ab8d97c7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#coefs = ....\n",
    "### BEGIN SOLUTION\n",
    "coefs = pipeline.named_steps['model'].coef_\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8810e2d4",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f42cbedb988b6aa8",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert coefs.shape == (30,), 'Wrong number of coefficients. Did you select the features correctly?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3facfa",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-57f2ca220627e218",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Exercises complete, congratulations! You are about to become a certified data wrangler."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
