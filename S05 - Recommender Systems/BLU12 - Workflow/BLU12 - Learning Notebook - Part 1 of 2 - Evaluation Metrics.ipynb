{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLU12 - Learning Notebook 1 - Evaluation Metrics for Recommender Systems\n",
    "\n",
    "You have already learned about metrics in *SLU08 - Metrics for Regression* and *SLU10 - Metrics for Classification* and you have successfully applied some of them in hackathons. What kind of metrics should we use to evaluate the performance of recommenders? Regression metrics? Classification metrics? It depends on what is the goal of the analysis and how the data is processed.\n",
    "\n",
    "The three types of metrics available for recommender evaluations are:\n",
    "\n",
    "1. Regression metrics\n",
    "\n",
    "2. Classification metrics\n",
    "\n",
    "3. Information retrieval metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "[1. Regression metrics](#1-Regression-metrics)   \n",
    "[2. Classification metrics](#2.-Classification-metrics)   \n",
    "[3. Information retrieval](#3.-Information-retrieval)   \n",
    "&emsp;[3.1 Information retrieval evaluation metrics for top-N lists](#3.1-Information-retrieval-evaluation-metrics-for-top-N-lists)   \n",
    "&emsp;&emsp;[3.1.1 Precision](#3.1.1-Precision)   \n",
    "&emsp;&emsp;[3.1.2 Recall](#3.1.2-Recall)   \n",
    "&emsp;&emsp;[3.1.3 Precision and recall @k](#3.1.3-Precision-and-recall-@k)   \n",
    "&emsp;&emsp;[3.1.4 Precision-recall curve](#3.1.4-Precision-recall-curve)   \n",
    "&emsp;&emsp;[3.1.5 Average precision](#3.1.5-Average-precision)   \n",
    "&emsp;&emsp;[3.1.6 Mean average precision](#3.1.6-Mean-average-precision)   \n",
    "&emsp;&emsp;[3.1.7 Spearman's rank correlation coefficient](#3.1.7-Spearman's-rank-correlation-coefficient)   \n",
    "[4. Final remarks](#4.-Final-remarks)   \n",
    "[5. Further reading](#5.-Further-reading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regression metrics\n",
    "\n",
    "It is possible to conceptualize a recommendation problem as a regression problem if the objective of the analysis is to predict the value of a numerical indicator such as ratings. If you want to predict the value of a given indicator for a specific item and user, you can - theoretically - solve it as a regression problem. By defining user and item variables as independent variables and the respective rating as the dependent variable (like in the example below), it is possible to solve it with regression models. \n",
    "\n",
    "Consider the case where movie rating data is processed to obtain the following table:\n",
    "\n",
    "<img src=\"./media/regression.png\" width=\"650\" />\n",
    "\n",
    "The user and item variables are the independent variables and the rating is the dependent variable and thus it is possible to use regression models to predict the movie rating for a specific user. The results can be evaluated using regression metrics as discussed in *SLU08 - Metrics for Regression*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification metrics\n",
    "\n",
    "Similarly, a recommendation problem can be reframed as a classification problem if the target variable is categorical.\n",
    "\n",
    "If the goal is to predict if a user would recommend a given item or not, the data can be represented as variables and a classification model can be trained to make predictions. The quality of these predictions can be evaluated using classification metrics as described in *SLU10 - Metrics for Classification*.\n",
    "\n",
    "<img src=\"./media/classification.png\" width=\"550\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Information retrieval\n",
    "\n",
    "From [*Introduction to Information Retrieval*](https://www-nlp.stanford.edu/IR-book/):\n",
    "\n",
    "\"Information retrieval (IR) is finding material (usually documents) of an unstructured nature (usually text) that satisfies an information need from within large collections (usually stored on computers).\" \n",
    "\n",
    "Basically when using search engines or asking for advice from a librarian, we are engaging in IR. We are using search terms to perform a query over a large collection of resources. We won't go into much detail on IR. What is important to us is how a recommender system compares to an information retrieval system (IRS) and how an IRS evaluates the accuracy of the results.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"./media/IRS_RS.png\" width=\"700\" /></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "In simple terms, an IRS takes a user query and tries to return documents that contain relevant information. Similarly, a recommender system tries to find items that satisfy the user. IRS usually deals with queries created by the user while with RS there isn't a direct query from the user.\n",
    "\n",
    "We can think of it in the following manner: a recommender system is an information retrieval system that answers the query \"What items best satisfy this user?\".\n",
    "The RS will provide the results of this query (the recommendations) and the user will provide feedback by accessing (or not) the items or by providing a rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IR metrics are used to compare the results provided by the IRS and the expected results for the given query.\n",
    "Two factors indicate how good are the IR results: **rank** and **relevance**.\n",
    "\n",
    "The IR results are provided as a ranked list of documents starting with the documents that best fit the query criteria. \n",
    "Consider an example where we use a search engine to find web pages regarding regression - as in regression analysis. For us, a web page with detailed information on regression analysis is more useful than a page that simply states the definition of regression or a page about age regression or the movie *Regression (2015)*. This degree of \"usefulness\" can be used to rank the results - provided there is feedback. \n",
    "\n",
    "A relevant document (or result) is a document that is expected by the user. Using the previous example, the pages about age regression and the movie are not relevant to the user. Having a high number of non-relevant results is an indicator of poor IR performance.\n",
    "\n",
    "When a RS returns a ranked list of results just like an IRS, it is possible to use IR metrics to evaluate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Information retrieval evaluation metrics for top-N lists\n",
    "\n",
    "Let's start by introducing some nomenclature with a dummy example. Imagine a user that has preference for fruits. We develop a RS model that creates a list of recommendations ranked from most recommended to least recommended, resulting in:\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"./media/IR_sets.png\" width=\"800\" /></center>\n",
    "<center> The sets' elements are ordered by order of preference.</center>\n",
    "<br>\n",
    "\n",
    "The first row represents the actual user preferences (as avaliable in test data) ranked by order of preference and the second row represents the recommendations also sorted by predicted preference. The set of all user preferences is represented as $\\{Relevant\\ items\\}_u$ and the set of all recommendations is represented as $\\{Retrieved\\ items\\}_u$. The relevant items are highlighted in green and the non-relevant items (and therefore not present in $\\{Relevant\\ items\\}_u$) are highlighted in red.\n",
    "\n",
    "\n",
    " Additionally, $|{set}|$ is the __cardinality__ of the set, which in this case is the number of items in the set. The operator $\\cap$ represents the __intersection__ between two sets. The result of the intersection between two sets is another set whose elements are present in both original sets.\n",
    "\n",
    "With this out of the way, let's define some evaluation metrics.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Precision\n",
    "\n",
    "Precision measures how many recommended items for user $u$ are relevant.\n",
    "\n",
    "$$Precision\\ (\\{Retrieved\\ items\\}_u) = \\frac{|\\{Retrieved\\ items\\}_u \\cap \\{Relevant\\ items\\}_u |}{|\\{Retrieved\\ items\\}_u|}$$\n",
    "\n",
    "To evaluate the RS as a whole, we average the precision for all active users $u \\in U$, where $U$ is the set containing all users.\n",
    "\n",
    "$$Precision\\ (\\{Retrieved\\ items\\}) = \\frac{\\sum\\limits_{u \\in U} Precision\\ (\\{Retrieved\\ items\\}_u)}{|U|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Recall\n",
    "\n",
    "Recall, on the other side, relates to how many relevant items were recommended, out of all relevant items for the user $u$.\n",
    "\n",
    "$$Recall\\ (\\{Retrieved\\ items\\}_u) = \\frac{|\\{Retrieved\\ items\\}_u \\cap \\{Relevant\\ items\\}_u |}{|\\{Relevant\\ items\\}_u|}$$\n",
    "\n",
    "Again, to evaluate the RS we average the results for all active users $u \\in U$.\n",
    "\n",
    "$$Recall\\ (\\{Retrieved\\ items\\}) = \\frac{\\sum\\limits_{u \\in U} Recall\\ (\\{Retrieved\\ items\\}_u)}{|U|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Precision and recall @k\n",
    "\n",
    "For applications such web searches, it is not interesting to consider all retrieved results. Rather the user is interested in the first k results (from the first page or the first n pages). In line with this, both precision and recall can be calculated only up to a defined rank (or cut-off) k - precision@k and recall@k.\n",
    "\n",
    "From the N retrieved items, we consider only the first k recommendations from rank 1 to rank k, $\\{Retrieved\\ Items\\}_u^k$:\n",
    "\n",
    "$$Precision@k\\ (\\{Retrieved\\ items\\}_u) = \\frac{|\\{Retrieved\\ items\\}_u^k \\cap \\{Relevant\\ items\\}_u |}{|\\{Retrieved\\ items\\}_u^k|}$$\n",
    "\n",
    "$$Recall@k\\ (\\{Retrieved\\ items\\}_u) = \\frac{|\\{Retrieved\\ items\\}_u^k \\cap \\{Relevant\\ items\\}_u |}{|\\{Relevant\\ items\\}_u|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 Precision-recall curve\n",
    "\n",
    "Plotting precision vs recall, we get the precision-recall curve (PRC), similar to the ROC curve (recall or true positive rate vs the false positive rate). PRC measures the capacity of the model to identify many relevant items without select many non-relevant items. If the (k + 1)th document retrieved is nonrelevant then recall is the same as for the top k documents, but precision has dropped. If it is relevant, then both precision and recall increase,  and the curve jags up and to the right.\n",
    "\n",
    "<img src=\"./media/AUPRC_balanced.png\" width=\"500\" />\n",
    "<center> A Precision-recall curve for a logistic regression on a <b>balanced</b> dataset <a href=\"https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\">(Source)</a>. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the area under the curve - AUPRC. Note that the value of the AUPRC for a no skill model <b>is equal to the proportion of positive cases</b> when $N$ is equal to the number of all available items ([source](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118432)). This contrasts with the fact that AUROC of a no skill model is always 0.5 regardless of the proportion of positive cases.\n",
    "\n",
    "$$AUPRC_u = \\sum\\limits_{k=1}^NPrecision@k(\\{Retrieved\\ items\\}_u) * \\Delta Recall@k(\\{Retrieved\\ items\\}_u)$$ \n",
    "\n",
    "The AUPRC can be particularly useful when the fraction of positive cases is small and when identifying a positive case is more important than identifying negative cases. These conditions typically happen in medical applications where the incidence of a condition is low - low number positive cases - and is more important to identify the condition than to verify that the pacient does not have the condition. In RS these conditions also apply. The users can only access a tiny amount of all available items; just consider the amount of books, movies, series, products, etc. The amount of relevant items is, therefore, significantly smaller than non-relevant items. Furthermore, it is more important to recommend relevant items than to make sure that non-relevant items are not recommended. The user might even like the - supposedly - non-relevant item after all. Take the case of [Plastic Love - Mariya Takeuchi](https://www.youtube.com/watch?v=3bNITQR4Uso). In 2018, the youtube recommendation algorithm \"spontaneously\" recommended an 80's pop song to a massive number of users that were completely unaware of the City Pop genre. This sparked the resurgence of an entire music genre ([Source](https://www.youtube.com/watch?v=PlPTXR7e6As)).\n",
    "\n",
    "<img src=\"./media/failed_success.jpeg\" width=\"400\" />\n",
    "<center> When recommending something weird and the user likes it.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 Average precision\n",
    "The average precision approximates the area under PRC. Average precision is the average of the precision values obtained for the set of top k items after each relevant item for the user is retrieved:\n",
    "\n",
    "$$AP@N\\ (\\{Retrieved\\ items\\}_u) = \\frac{\\sum\\limits_{k = 1}^N (Precision@k\\ (\\{Retrieved\\ items\\}_u) \\cdot relevant(k^{th})}{|\\{Relevant\\ items\\}_u|}$$\n",
    "\n",
    "N is the total number of retrieved items. The $relevant(k^{th})$ bit is a boolean value, indicating whether the $k$-th item is relevant or not (so effectively, the sum contains only the terms for relevant items).\n",
    "\n",
    "AP takes into account both the rank and the number of relevant items. It increases only with correct recommendations and ignores non-relevant items. Early hits, i.e. front-ranking correct recommendations, carry over and are continuously rewarded. Therefore, AP can never decrease as you increase $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.6 Mean average precision\n",
    "\n",
    "The average precision can be further averaged over all users (all queries in case of an IRS):\n",
    "\n",
    "$$mAP@N(\\{Retrieved\\ items\\}) = \\frac{\\sum\\limits_{u \\in U} AP@N(\\{Retrieved\\ items\\}_u)}{|U|}$$\n",
    "\n",
    "Here, *mean* refers to averaging over users/queries and *average* to averaging over items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.7 Spearman's rank correlation coefficient\n",
    "\n",
    "Another approach is to consider the rank of the relevant items as one variable and the rank of the recommendations as another. In this case it is appropriate to use rank correlation coefficients to evaluate the **ordinal** relationship between them. One of these rank correlation coefficients was already discussed in SLU05 - Covariance and Correlation: the **Spearman's rank correlation coefficient, $r_s$**. \n",
    "\n",
    "To calculate the rank correlation, the **rank** of each item in $\\{Relevant\\ items\\}_u$ is compared with the **rank of the same item** in $\\{Retrieved\\ items\\}_u$. We can define two rank variables: user's preferences ranks (*PR*) and recommendations ranks (*RR*), where the user's and recommendations' ranks for each item are stored, respectively. We are then comparing the position of each item in both variables. What matters is that the **position of an item has to be the same in both variables. If the first element of PR is the user's preferences rank of \"Item X\" then the first element of RR has to be the recommendations rank of \"Item X\" and so forth.**\n",
    "\n",
    "Considering the case above, we can identify the rank of each item of $\\{Relevant\\ items\\}_u$ and $\\{Retrieved\\ items\\}_u$. By reordering the RR to have the items on the same position as PR it is easier to see that matching items can have different rank value between PR and RR. For instance, the item \"apple\" is the user's second (2) preferred item while it is the fifth (5) recommended item.\n",
    "<img src=\"./media/spearman_table.png\" width=\"800\" />\n",
    "<center>The items' rank is written within paranthesis ().</center>\n",
    "\n",
    "The Pearson correlation between these two variables returns the Spearman's rank correlation coefficient between recommendations and the user's preference.\n",
    "When all the ranks are distinct integers - such as in our case - the Spearman's rank correlation coefficient can be calculated with the formula:\n",
    "\n",
    "$$ r_s = 1- \\frac{6 \\sum\\limits_{i=1}^{n} d_i^2}{n(n-1)}$$\n",
    "\n",
    "where $i$ iterates over the items, $n$ is the number of ranks (items) to be compared and $d_i = PR_i - RR_i$ is the difference of ranks for item $i$. In our case $n$ can be the cut off $k$ on the number of relevant items. \n",
    "\n",
    "The Spearman's rank correlation coefficient can have values ranging between -1 and 1. Correlation of 1 indicates that the order of the ranks in both variables is exactly the same, while correlation of -1 indicates that the variables have fully opposing order. Correlation of 0 indicates that the variables are completely independent. One should note that the computation of the correlation is vulnerable to the presence of missing items, mainly in $\\{Retrieved\\ items\\}_u$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final remarks\n",
    "\n",
    "There are a lot more metrics that can be used to evaluate the outcomes of recommender systems. The preference from one metric to another depends on multiple factors such as output type (binary, ordinal, recommendations), the type of users' feedback, business requirements, and ease of interpretation. We have explored some of the common metrics for recommender systems. It is encouraged to explore other metrics. The best metric for a specific context might not be helpful for another context.\n",
    "\n",
    "## Time to practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Further reading\n",
    "\n",
    "- [*Short introduction to Information retrieval*](https://www.tutorialspoint.com/natural_language_processing/natural_language_processing_information_retrieval.htm)\n",
    "\n",
    "- [*Introduction to Information Retrieval*](https://nlp.stanford.edu/IR-book/), more specifically the [*chapter 8*](https://nlp.stanford.edu/IR-book/pdf/08eval.pdf) on evaluation.\n",
    "\n",
    "- [*Information Retrieval Wiki*](https://en.wikipedia.org/wiki/Information_retrieval)\n",
    "\n",
    "- [*Information Retrieval Metrics Wiki*](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval))\n",
    "\n",
    "- [*Precision-Recall Curves in Python*](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/)\n",
    "\n",
    "- [*More info on AUPRC*](https://glassboxmedicine.com/2019/03/02/measuring-performance-auprc/)\n",
    "\n",
    "- [*Rank Correlation Wiki*](https://en.wikipedia.org/wiki/Rank_correlation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
