{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "### Introduction\n",
    "\n",
    "As you may have noticed, this set of BLUs will revolve around the topic of Natural Language Processing (NLP). As the name implies, this field is all about the processing and handling of language in such a way that a computer may be able to do useful things with it. There are plenty of tasks and problems around it, namely:\n",
    "\n",
    "- **Speech recognition**: the task of, given a sample of audio, extract the words that are being spoken or even prosody features, for example.\n",
    "- **Natural language generation**: the task of putting computational formulations into actual text, for example, automated generation of labels to images, summarisation of texts and data, creation of dialogue systems, etc.\n",
    "- **Natural language understanding**: the task of getting some meaning out of the data, for instance, recognizing entities in sentences, semantic roles, or even classify sentences according to their sentiment, etc., or transforming it into something machines can work on (numbers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More formally, some of the main tasks and areas of research of NLP are:\n",
    "\n",
    "- **Part of Speech tagging**: Determine the role of each word in a given sentence, for instance, if it is an adjective, verb, noun, etc.\n",
    "\n",
    "- **Word Segmentation**: Break continuous text into words.\n",
    "\n",
    "- **Parsing**: Define a tree that represents the grammatical structure of a sentence.\n",
    "\n",
    "- **Machine Translation**: Translate sentences from a source language to a target language automatically.\n",
    "\n",
    "- **Named entity recognition**: Find parts of the text that correspond to certain entities, like names of places, people, companies, etc.\n",
    "\n",
    "- **Question answering**: Given a question in human language, find the most appropriate answer.\n",
    "\n",
    "- **Text to speech**: As the name implies, transform written text into audible, human-like sounds that correspond to the given input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these tasks are out of the scope of these learning units, but we think that it is important to at least acknowledge that they exist in the realm of NLP. \n",
    "\n",
    "\n",
    "### Processing text \n",
    "\n",
    "Most areas that use text data require in one way or the other that it is transformed into more useful input. Some of these transformations may be, for example:\n",
    "\n",
    "- Splitting words in a sentence\n",
    "- Removing punctuation\n",
    "- Removing common words\n",
    "- Extracting suffixes or prefixes\n",
    "\n",
    "When you first hear about these text processing tasks you are learning, for example separating the words in a sentence, it may seem _easy enough_. After all, words are separated by spaces or maybe some punctuation. But when you really think about the diversity that exists in terms of languages you start to understand how daunting all these tasks are. Take a look at Mandarin Chinese, for instance. Our heuristic is suddenly not valid anymore. And for many of the tasks, there are plenty of corner cases.\n",
    "\n",
    "<img src=\"./media/xkcd_language_nerd.png\" width=\"300\">\n",
    "\n",
    "Bottom line is, language is hard, but that's what makes this field one of the most challenging but also more rewarding to work on.\n",
    "\n",
    "\n",
    "Throughout these learning units we hope to give you some basic understanding on how to transform text into something useful for us, explain what are some of the challenges in this field, asolve some interesting problems and hopefully make you want to learn more about the topic afterwards!\n",
    "\n",
    "The first part of this BLU goes through some of the fundamental concepts that will be helpful for all the practical tasks that you will need during this month, but also in the future, if you ever need to work with text data. We will start by introducing **regular expressions**, followed by three important concepts in data pre-processing (**tokenization**, **stopwords**, and **stemming**). Finally, we will learn about **n-grams** and **n-gram-models**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding patterns in text\n",
    "\n",
    "To be able to perform any of the text processing tasks mentioned in an efficient way, we need to be able to parse and detect patterns in the text programatically. For the purpose of simplification, we'll only focus on English for the next couple of examples.\n",
    "\n",
    "Let's say you have the following text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_example = \"\"\"\n",
    "Bommer Canyon is an open space preserve in southern Irvine, California featuring hiking and \n",
    "biking trails as well as private event areas. The canyon is part of the Irvine Ranch, which \n",
    "itself is a National Natural Landmark, the first California Natural Landmark,[1][2] and part \n",
    "of the City of Irvine Open Space Preserve.[3][4] The preserve is adjacent to the affluent \n",
    "Irvine villages of Shady Canyon and Turtle Ridge and features roughly 16,000 acres of \n",
    "preserved open space.[5] Approximately 15 of these acres are preserved as a \"Cattle Camp\" \n",
    "named for the area's previous cattle operations and are now rented for private events such \n",
    "as campouts, company picnics, and family reunions.[6] The trails in Bommer Canyon feature \n",
    "groves of oak and sycamore trees as well as rough rock outcrops and are popular with area \n",
    "residents who use them for nature walks, hiking and mountain biking.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you answer the question:\n",
    "\n",
    "> What are all the words in the following text that start with the letter `a`?\n",
    "\n",
    "You could obviously count them manually, but if you imagine that you can have thousands or millions of lines, that becomes impossible. \n",
    "\n",
    "A second option, given your recently acquired skills in python, could be to write a function that does this for you:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words found that start with 'a':\n",
      "- an\n",
      "- and\n",
      "- as\n",
      "- as\n",
      "- areas.\n",
      "- a\n",
      "- and\n",
      "- adjacent\n",
      "- affluent\n",
      "- and\n",
      "- and\n",
      "- acres\n",
      "- acres\n",
      "- are\n",
      "- as\n",
      "- a\n",
      "- area's\n",
      "- and\n",
      "- are\n",
      "- and\n",
      "- and\n",
      "- as\n",
      "- as\n",
      "- and\n",
      "- are\n",
      "- area\n",
      "- and\n"
     ]
    }
   ],
   "source": [
    "def find_all_words_starting_with_a(text):\n",
    "    \n",
    "    # Assuming all words can be split by spaces - big assumption\n",
    "    list_of_a_words = []\n",
    "    words = text.split(\" \")\n",
    "    for w in words:\n",
    "        if w.startswith(\"a\"):\n",
    "            list_of_a_words.append(w)\n",
    "    \n",
    "    return list_of_a_words\n",
    "\n",
    "print(\"Words found that start with 'a':\")\n",
    "for w in find_all_words_starting_with_a(text_example):\n",
    "    print(\"- \" + w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a slightly more complex task: we want to find and remove all punctuation, replacing it with a space when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bommer Canyon is an open space preserve in southern Irvine California featuring hiking and \n",
      "biking trails as well as private event areas The canyon is part of the Irvine Ranch which \n",
      "itself is a National Natural Landmark the first California Natural Landmark  1  2 and part \n",
      "of the City of Irvine Open Space Preserve  3  4 The preserve is adjacent to the affluent \n",
      "Irvine villages of Shady Canyon and Turtle Ridge and features roughly 16 000 acres of \n",
      "preserved open space  5 Approximately 15 of these acres are preserved as a Cattle Camp \n",
      "named for the area s previous cattle operations and are now rented for private events such \n",
      "as campouts company picnics and family reunions  6 The trails in Bommer Canyon feature \n",
      "groves of oak and sycamore trees as well as rough rock outcrops and are popular with area \n",
      "residents who use them for nature walks hiking and mountain biking \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation(text):\n",
    "    \n",
    "    big_list_of_punctuation = [\n",
    "        \".\", \",\", \"?\", \"!\", \"-\", \"_\", \":\", \";\",\n",
    "        \"\\\"\", \"'\", \"|\", \"(\", \")\", \")\", \"/\", \"\\\\\",\n",
    "        \"[\", \"]\",\n",
    "    ]\n",
    "    \n",
    "    processed_text = \"\"\n",
    "    for idx, letter in enumerate(text):\n",
    "        previous_letter = text[idx - 1] if idx > 1 else \"\"\n",
    "        next_letter = text[idx + 1] if idx < len(text) - 2 else \"\"\n",
    "        if letter in big_list_of_punctuation:\n",
    "            if previous_letter != \" \" and next_letter != \" \":\n",
    "                processed_text += \" \"\n",
    "        else:\n",
    "            processed_text += letter\n",
    "        previous_letter = letter\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "print(remove_punctuation(text_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly more complicated, but seems to have worked! Now let's see what would happen for a different text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adenanthos venosus is an openly branched shrub that typically grows to a height of 1–2 m \n",
      " 3 ft 3 in – 6 ft 7 in and forms a lignotuber Its leaves are mostly arranged in clusters \n",
      "at the ends of branches egg shaped sometimes with the narrower end towards the base \n",
      "mostly 15–20 mm 0 59–0 79 in long 10 mm 0 39 in wide and sessile The leaves are \n",
      "mostly glabrous and have a pointed tip The flowers are ”dull crimson” to ”pinkish purple” \n",
      "with a cream coloured band in the centre and many glandular hairs on the outside \n",
      "\n"
     ]
    }
   ],
   "source": [
    "different_text_example = \"\"\"\n",
    "Adenanthos venosus is an openly-branched shrub that typically grows to a height of 1–2 m \n",
    "(3 ft 3 in – 6 ft 7 in) and forms a lignotuber. Its leaves are mostly arranged in clusters \n",
    "at the ends of branches, egg-shaped, sometimes with the narrower end towards the base, \n",
    "mostly 15–20 mm (0.59–0.79 in) long, 10 mm (0.39 in) wide and sessile. The leaves are \n",
    "mostly glabrous and have a pointed tip. The flowers are ”dull crimson” to ”pinkish purple” \n",
    "with a cream-coloured band in the centre and many glandular hairs on the outside. \n",
    "\"\"\"\n",
    "\n",
    "print(remove_punctuation(different_text_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like this time we missed a bit, in particular the `”` characters. \n",
    "\n",
    "Of course we could go back and re-write our function but what we want to point out is that coding all these tasks from scratch is not only inefficient, but also quite boring. Just for quoting for example, there are several potential characters that can be used, as can be seen [here](https://op.europa.eu/en/web/eu-vocabularies/formex/physical-specifications/character-encoding/quotation-marks).\n",
    "\n",
    "And we may have endless different variations and extra conditions that will make everything hard to keep track. Some examples just off the top of my head:\n",
    "\n",
    "- Replacing all numbers with a placeholder\n",
    "- Remove all decimals from numbers\n",
    "- Count all uppercase letters in a text\n",
    "- Find all words that have less than 3 letters (or any other number)\n",
    "- ...\n",
    "\n",
    "This is where **regular expressions** come in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions (aka Regex)\n",
    "\n",
    "Regular expressions are sequences of characters that allow us to define search patterns in a standardized way. It goes by several rules and is one of the most fundamental concepts in computer science regarding working with text data.\n",
    "\n",
    "Most of the tasks that we defined before can be performed with it. Let's take our first example, finding all words starting with 'a' in the text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words found that start with 'a':\n",
      "- an\n",
      "- and\n",
      "- as\n",
      "- as\n",
      "- areas\n",
      "- and\n",
      "- adjacent\n",
      "- affluent\n",
      "- and\n",
      "- and\n",
      "- acres\n",
      "- acres\n",
      "- are\n",
      "- as\n",
      "- area\n",
      "- and\n",
      "- are\n",
      "- as\n",
      "- and\n",
      "- and\n",
      "- as\n",
      "- as\n",
      "- and\n",
      "- are\n",
      "- area\n",
      "- and\n"
     ]
    }
   ],
   "source": [
    "def find_all_words_starting_with_a_regex(text):\n",
    "    \n",
    "    pattern = r\"\\ba\\w+\\b\"\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "print(\"Words found that start with 'a':\")\n",
    "for w in find_all_words_starting_with_a_regex(text_example):\n",
    "    print(\"- \" + w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much more compact, right? Now let's do the same for punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bommer Canyon is an open space preserve in southern Irvine California featuring hiking and biking trails as well as private event areas The canyon is part of the Irvine Ranch which itself is a National Natural Landmark the first California Natural Landmark 1 2 and part of the City of Irvine Open Space Preserve 3 4 The preserve is adjacent to the affluent Irvine villages of Shady Canyon and Turtle Ridge and features roughly 16 000 acres of preserved open space 5 Approximately 15 of these acres are preserved as a Cattle Camp named for the area s previous cattle operations and are now rented for private events such as campouts company picnics and family reunions 6 The trails in Bommer Canyon feature groves of oak and sycamore trees as well as rough rock outcrops and are popular with area residents who use them for nature walks hiking and mountain biking \n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation_regex(text):\n",
    "    \n",
    "    pattern_punkt = r\"[\\.,\\?\\!\\-\\_\\:\\;\\\"'\\|\\(\\)/\\\\\\[\\]]\"\n",
    "    \n",
    "    # Replaces all punctuation characters by spaces\n",
    "    text_no_punkt = re.sub(pattern_punkt, \" \", text)\n",
    "    \n",
    "    # Collapses multiple spaces\n",
    "    text_no_punkt = re.sub(r\"\\s+\", \" \", text_no_punkt)\n",
    "\n",
    "    return text_no_punkt\n",
    "    \n",
    "\n",
    "print(remove_punctuation_regex(text_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool right? \n",
    "\n",
    "Regex enables us to do a bunch of interesting things in text, and much more complex things. But don't just trust our word for it. Actually try these out. You can use [this website](https://regexr.com/3lvai) to play around with regex and validate that the patterns you wrote are performing as expected. See some extra examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find digits\n",
    "pattern_digits = \"[0-9]*\"\n",
    "\n",
    "# Find words smaller than 3 characters \n",
    "pattern_words_until_3 = \"\\b\\w{1,3}\\b\"\n",
    "\n",
    "# Find URLs in a text\n",
    "pattern_urls = \"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now at this point you may be looking a bit like this\n",
    "\n",
    "<img src=\"./media/regex-confusion.png\" width=\"500\">\n",
    "\n",
    "But worry not! \n",
    "\n",
    "Most of us don't know regex by heart, and we need to take a look at cheatsheets from time to time, like the one shown below. Still, as you move forward in the NLP world, we hope you see how helpful regex can be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cheatsheet [\\[1\\]](https://regexr.com/3lvai)\n",
    "\n",
    "`.` - matches any character, except newline.\n",
    "\n",
    "`\\d, \\s \\S` - match digit, match whitespace, not whitespace.\n",
    "\n",
    "`\\b, \\B` - word, not word boundary.\n",
    "\n",
    "`[xyz]` - matches x, y or z.\n",
    "\n",
    "`[^xyz]` - matches anything that is not x, y or z.\n",
    "\n",
    "`[x-z]` - matches a character between x and z.\n",
    "\n",
    "`^xyz$` - `^` is the start of the string, `$` is the end of the string.\n",
    "\n",
    "`\\.` - use escaping to match special characters.\n",
    "\n",
    "`\\t`, `\\n` - matches tab and newline.\n",
    "\n",
    "`x*` - matches 0 or more symbols x.\n",
    "\n",
    "`x+` - matches 1 or more symbols x.\n",
    "\n",
    "`x?` - matches 0 or 1 symbol x.\n",
    "\n",
    "`.?`, `*?`, `+?`, etc - represent non-greedy search. \n",
    "\n",
    "`x{5}` - matches exactly 5 symbols x.\n",
    "\n",
    "`x{5,}` - matches 5 or more symbols x.\n",
    "\n",
    "`x{5, 8}` - matches between 5 and 8 symbols x.\n",
    "\n",
    "`xy|yz` - matches `xy` or `yz`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python `re` library\n",
    "\n",
    "The python library that we are using - [re](https://docs.python.org/3/library/re.html) - has many powerful methods too. We'll deep dive into each of them in the following sections.\n",
    "\n",
    "### Finding one instance with `search()` \n",
    "\n",
    "Using `search()` we can take a certain pattern and look for it in a text. This function will return a `Match` object, from which we can obtain the first text portion that was matched by our pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for \"Madrid\":\n",
      "<re.Match object; span=(7, 13), match='Madrid'>\n",
      "\n",
      "Looking for \"Rome\":\n",
      "None\n",
      "\n",
      "Looking for \"Lisbon\":\n",
      "<re.Match object; span=(0, 6), match='Lisbon'>\n"
     ]
    }
   ],
   "source": [
    "text = \"Lisbon Madrid Lisbon Toulose Oslo Lisbona\"\n",
    "\n",
    "print(\"Looking for \\\"Madrid\\\":\")\n",
    "match = re.search(\"Madrid\", text)\n",
    "print(match)\n",
    "\n",
    "print(\"\\nLooking for \\\"Rome\\\":\")\n",
    "match = re.search(\"Rome\", text)\n",
    "print(match)\n",
    "\n",
    "print(\"\\nLooking for \\\"Lisbon\\\":\")\n",
    "match = re.search(\"Lisbon\", text)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it is already possible to observe some things about `re.search()`:\n",
    "\n",
    "- When there is no match, `search()` returns `None`.\n",
    "\n",
    "- The `Match` object has the index of the beginning and end of the match. Might be used via `match.start()` and `match.end()`.\n",
    "\n",
    "- If there is more than one instance of the word in the text, only the first will be retrieved.\n",
    "\n",
    "### Finding all instances with `findall()`  or `finditer()`\n",
    "\n",
    "If we want to return all the matches to our pattern in a given text we might use the function `findall()`. In this case, the matched portions of the text will be returned, instead of the `Match` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lisbon\n",
      "Lisbon\n",
      "Lisbon\n"
     ]
    }
   ],
   "source": [
    "text = \"Lisbon Madrid Lisbon Toulose Oslo Lisbona\"\n",
    "\n",
    "pattern = \"Lisbon\"\n",
    "\n",
    "for match in re.findall(pattern, text):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, one of the words was written as _Lisbona_ , but we still match the _Lisbon_ portion of that word. If we add the condition of having a white space after the letter *n* we will only get two matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lisbon \n",
      "Lisbon \n"
     ]
    }
   ],
   "source": [
    "pattern = \"Lisbon\\s\"\n",
    "\n",
    "for match in re.findall(pattern, text):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If instead we really want the `Match` objects for some reason, `finditer()` should be used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 6), match='Lisbon'>\n",
      "<re.Match object; span=(14, 20), match='Lisbon'>\n",
      "<re.Match object; span=(34, 40), match='Lisbon'>\n"
     ]
    }
   ],
   "source": [
    "pattern = \"Lisbon\"\n",
    "\n",
    "for match in re.finditer(pattern, text):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing all instances with `sub()`  or `\n",
    "\n",
    "If we want to replace the matches of our pattern in a given text with something else we need to use the function `sub()`. In this case, the matched portions of the text will be replaced, and the changed text will be returned.\n",
    "\n",
    "For example if we wanted to remove the word `Lisbon` from a text we could do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Madrid  Toulose Oslo Lisbona\n"
     ]
    }
   ],
   "source": [
    "text = \"Lisbon Madrid Lisbon Toulose Oslo Lisbona\"\n",
    "\n",
    "# \\b indicates a word boundary so using it around a word \n",
    "# will only replace the text when it shows as a proper word, \n",
    "# between spaces or punctuation \n",
    "pattern = r\"\\bLisbon\\b\"\n",
    "\n",
    "print(re.sub(pattern, \"\", text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If instead we wanted to replace by another word we could just specify it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lisboa Madrid Lisboa Toulose Oslo Lisbona\n"
     ]
    }
   ],
   "source": [
    "text = \"Lisbon Madrid Lisbon Toulose Oslo Lisbona\"\n",
    "\n",
    "# \\b indicates a word boundary so using it around a word \n",
    "# will only replace the text when it shows as a proper word, \n",
    "# between spaces or punctuation \n",
    "pattern = r\"\\bLisbon\\b\"\n",
    "\n",
    "print(re.sub(pattern, \"Lisboa\", text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### A primer on patterns\n",
    "\n",
    "Now that you are familiar with the `re` functions, we'll use them to explore a bit better the paterns possible with regex\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at some of the previously shown codes at cheatsheet, let's see in some simple examples how that may help us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"x xy xyy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remembering what we've shown previously, `.` will match any character after x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x ', 'xy', 'xy']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"x.\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`*` will match 0 or more y symbols after xy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x', 'xy', 'xyy']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"xy*\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`+` will match 1 or more y symbols after x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xy', 'xyy']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"xy+\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`?` will match 0 or 1 y symbols after x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x', 'xy', 'xy']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"xy?\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`{i}` will match i y symbols after x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xyy']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"xy{2}\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"lotterer Jani Senna conway Kobayashi Lopez buemi Nakajima alonso\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to match only the names that start with capital letters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jani', 'Senna', 'Kobayashi', 'Lopez', 'Nakajima']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[A-Z][a-z]+\", text) # find substrings starting with a capital letter\n",
    "                                # followed by 1 or more lowercase letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to match all the names that don't start with letters \"B\" and \"L\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jani', 'Senna', 'conway', 'Kobayashi', 'Nakajima', 'alonso']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"\\b[^bBlL\\s][A-Za-z]+\", text) # find substrings after a word boundary that...\n",
    "                                          # do not begin with B or L or whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be wondering what that hacky `r` is doing before the actual regex we are using. This has no connection with regex. It is just a way of telling python that it should interpret backslashes `\\` literally (Notice how our regex has `\\b` and `\\s`). For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With r:\n",
      "\n",
      "lotterer \\n Jani \\n Senna conway Kobayashi Lopez buemi Nakajima alonso\n",
      "\n",
      "\n",
      "Without r:\n",
      "\n",
      "lotterer \n",
      " Jani \n",
      " Senna conway Kobayashi Lopez buemi Nakajima alonso\n"
     ]
    }
   ],
   "source": [
    "print(\"With r:\\n\")\n",
    "print(r\"lotterer \\n Jani \\n Senna conway Kobayashi Lopez buemi Nakajima alonso\")\n",
    "print(\"\\n\")\n",
    "print(\"Without r:\\n\")\n",
    "print(\"lotterer \\n Jani \\n Senna conway Kobayashi Lopez buemi Nakajima alonso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first case, since we are using `r` the model takes `\\n` literally and in the second case, python interprets it as the escaped symbol for newline. Another important thing to know is that because regex interprets several characters in a special way, if you want to match them literally, you need to escape them. For that purpose you also use the `\\`. Whatever comes after this character is considered escaped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Text \\ with + special [characters].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Matches:\\n\")\n",
    "\n",
    "for m in re.findall(r\".+[ ]\\ \", text): # If we don't escape the characters we mean\n",
    "    print(m)                           # find, we won't match anything and could \n",
    "                                       # even have a broken regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches:\n",
      "\n",
      "\\\n",
      "+\n",
      "[\n",
      "]\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "print(\"Matches:\\n\")\n",
    "\n",
    "for m in re.findall(r\"[\\.\\+\\[\\]\\\\]\", text): # If we escape the characters we mean to \n",
    "    print(m)                                 # find, we'll match them literally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice in particular the `\\\\`. This is just a corner case, as the backlash is used to escape any character, it's also used to escape itself.  \n",
    "\n",
    "<img src=\"./media/backslashes.png\" width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Imagine now we have some extra information after the names, and that we receive a file with many lines. We still want only names starting with capital letters. So we run the previous regex and..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"lotterer Rebellion\\nJani Rebellion\\nSenna Rebellion\\nconway Toyota\\nKobayashi Toyota\\nLopez Toyota\\nbuemi Toyota\\nNakajima Toyota\\nalonso Toyota\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rebellion',\n",
       " 'Jani',\n",
       " 'Rebellion',\n",
       " 'Senna',\n",
       " 'Rebellion',\n",
       " 'Toyota',\n",
       " 'Kobayashi',\n",
       " 'Toyota',\n",
       " 'Lopez',\n",
       " 'Toyota',\n",
       " 'Toyota',\n",
       " 'Nakajima',\n",
       " 'Toyota',\n",
       " 'Toyota']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[A-Z][a-z]+\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we don't want those extra names in there. So let's try to add the symbol `^` to make sure the expression only captures the beginning part of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"^[A-Z][a-z]+\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hum.. we got a handful of nothing. Why is this happening? Well, the regex processes all the text as a single line, and the first name doesn't start with a capital letter. To make sure this is the case, let's change `lotterer` to `Lotterer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lotterer']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"Lotterer Rebellion\\nJani rebellion\\nSenna Rebellion\\nconway toyota\\nKobayashi Toyota\\nLopez Toyota\\nbuemi Toyota\\nNakajima toyota\\nalonso Toyota\"\n",
    "re.findall(\"^[A-Z][a-z]+\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we still only capture one line. Luckily, we have [`re.MULTILINE`](https://docs.python.org/3/library/re.html#re.MULTILINE), that allows us to process multiline strings easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lotterer', 'Jani', 'Senna', 'Kobayashi', 'Lopez', 'Nakajima']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"^[A-Z][a-z]+\", text, re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we were able to get all the information we wanted! And what if we wanted the second part of each line? Well, in this case, that is the last word of the line, so we may use `$`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rebellion', 'Rebellion', 'Toyota', 'Toyota', 'Toyota', 'Toyota']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[A-Z][a-z]+$\", text, re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want all full lines ending with `rebellion`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lotterer Rebellion', 'Jani rebellion', 'Senna Rebellion']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\".*rebellion$\", text, flags=(re.MULTILINE|re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that here we are also taking advantage of the flag `re.IGNORECASE`. This is a convenient flag to add if you want case-insensitive matches. Multiple regex flags can be strung together with pipes: `|`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions can get hard to read really fast, but even knowing the basics will be certainly helpful sometime in the future. To better understand how they work, nothing is better than practicing, and sites like [this](https://regexr.com/3lvai) and [this](https://regex101.com/) are valuable visual tools to do so. The python library that we used has a lot of more powerful methods too, which might be useful to future tasks.\n",
    "\n",
    "More suggestions for you to read about regex are:\n",
    "- https://towardsdatascience.com/regular-expressions-clearly-explained-with-examples-822d76b037b4\n",
    "- https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important step when dealing with text data is to _tokenize_ the data. In practice what this means is splitting the strings of a corpus into substrings. This is important because it transforms a string into parts that are more suitable to be used by the tools that exist in natural language processing. For instance, if we are working with the sentence:\n",
    "\n",
    "_\"The car went too fast on the second lap. This damaged the tires.\"_ ,\n",
    "\n",
    "would be better approached as a list,\n",
    "\n",
    "_[\"The\", \"car\", \"went\", \"too\", \"fast\", \"on\", \"the\", \"second\", \"lap\", \".\", \"This\", \"damaged\", \"the\", \"tires\", \".\"]_ .\n",
    "\n",
    "<img src=\"./media/tokenizer.png\" width=\"500\">\n",
    "\n",
    "Hopefully by now you've realized that this task is slightly more than just splitting spaces, and requires a bit more thought. To simplify things, there are already libraries and methods that help us implement tokenization in different ways.\n",
    "\n",
    "We will be using [NLTK](https://www.nltk.org/_modules/nltk/tokenize/regexp.html) implementations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The car went too fast on the second lap. This damaged the tires...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'car', 'went', 'too', 'fast', 'on', 'the', 'second', 'lap', '.', 'This', 'damaged', 'the', 'tires', '...']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the tokenizer is created by taking advantage of the regular expressions we learned earlier. This means that we can make different tokenizers according to what we want to split on. For instance, if we had used `[A-Z]\\w+`, the tokenizer would only select the words that begin with capital letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'This']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer('[A-Z]\\w+')\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are already some pre-defined implementations we can use by taking advantage of `RegexpTokenizer`. These are:\n",
    "- `BlanklineTokenizer` - Tokenize a string using blank lines as the delimiter.\n",
    "- `WordPunctTokenizer` - Tokenize a string into alphabetic and non-alphabetic characters.\n",
    "- `WhitespaceTokenizer`-  Tokenize a string using spaces, tabs, and newlines as delimiters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import BlanklineTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The car went too fast on the second lap. This damaged the tires...']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BlanklineTokenizer().tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'car',\n",
       " 'went',\n",
       " 'too',\n",
       " 'fast',\n",
       " 'on',\n",
       " 'the',\n",
       " 'second',\n",
       " 'lap',\n",
       " '.',\n",
       " 'This',\n",
       " 'damaged',\n",
       " 'the',\n",
       " 'tires',\n",
       " '...']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordPunctTokenizer().tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'car',\n",
       " 'went',\n",
       " 'too',\n",
       " 'fast',\n",
       " 'on',\n",
       " 'the',\n",
       " 'second',\n",
       " 'lap.',\n",
       " 'This',\n",
       " 'damaged',\n",
       " 'the',\n",
       " 'tires...']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WhitespaceTokenizer().tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `WordPunctTokenizer()` is similar to the first one we defined (`RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')`. This tokenizer is one of the most commonly used. So, when we talk about tokenization without specifying further details, it is by default the type of tokenization that we expect you to use (for example, in exercises).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Note**: Even though we are using NLTK library during this BLU, some other libraries are commonly used as well, and are probably better. Here is a list of some to consider in your future challenges in NLP:\n",
    "\n",
    "- [Spacy](https://spacy.io/)\n",
    "- [gensim](https://radimrehurek.com/gensim/)\n",
    "- [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/other-languages.html#python)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming allows us to get the \"root\" of words. This is important because in certain tasks we are more interested in a broader representation of a given word and not the specific variation of it, like its plural, for instance. Before using the stemmer it is necessary to download some tools required by `nltk`, regarding the language we want to use. We will be working with the English language, using the NLTK Downloader, the same way we would import `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/catarinasilva/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's see what this step gets us for the same example we have been using. To do that, we will be using the NLTK implementation of the [snowball stemmer](https://www.nltk.org/api/nltk.stem.html#nltk.stem.snowball.SnowballStemmer). Notice that there are other stemmers, some of them specific to certain tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'car', 'went', 'too', 'fast', 'on', 'the', 'second', 'lap', '.', 'this', 'damag', 'the', 'tire', '...']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "words = tokenizer.tokenize(text)\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "stems = [list(map(stemmer.stem, words))]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that _\"damage\"_ and _\"tires\"_ are transformed into simpler forms of the respective words. Notice as well that all the words have been lowercased. Lowercasing the data is also a common step in text pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords \n",
    "\n",
    "One thing that you may have noticed was the concept of \"stopwords\" being used. **Stopwords** are common words in a given corpus or language that, due to being so common, lose interest for most natural language processing applications. \n",
    "\n",
    "For instance, imagine a search engine, looking through a whole range of documents. Words as \"*the*\", \"*a*\", \"*at*\", etc. will be present in so many documents that using them in the search will not reduce the number of possible files that could be relevant to our query. So filtering them out is beneficial to our goal.\n",
    "\n",
    "In the specific case of the stemmer function that we are using, defining `ignore_stopwords` as `True` will prevent the stemming of stopwords.\n",
    "\n",
    "In the next part of this BLU you will read about stopwords again, as they are important for the task you will be doing there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides stemming there is also the process of **lemmatization**. Both processes share the goal of getting the root of the word, or more formally, reduce inflectional forms of a word to a common base form [\\[7\\]](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html), but they act differently. Whereas stemming follows a heuristic approach that drops the suffix of words in order to get closer to the common base form, lemmatization uses a dictionary and morphological analysis of words to return the base form of words, known as _lemma_.\n",
    "\n",
    "Using the example in the cited reference, if shown the word _saw_, stemming would tend to return only *s*, while lemmatization would take into account if the word was the verb or the noun, and correspondingly, return _see_ or _saw_  as the base form of the word.\n",
    "\n",
    "As you may expect, lemmatization is much more expensive in computational terms and, for certain applications, stemming might be more than enough to obtain good results. We will be using only stemming throughout the NLP learning units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_n-grams_ correspond to sequences of n consecutive elements from a given sentence. Commonly each element is a word, or \"token,\" but we may define it as we wish for the task at hand. Usually, we refer to unigrams, bigrams, trigrams, four-grams, etc. according to the length of the sequence of elements.\n",
    "\n",
    "For instance, for the sentence\n",
    "\n",
    "`\"The driver made a mistake\"`,\n",
    "\n",
    "we would have:\n",
    "\n",
    "- unigrams: `The`, `driver`, `made`, `a`, `mistake`\n",
    "- bigrams: `The driver`, `driver made`, `made a`, `a mistake`\n",
    "- trigrams: `The driver made`, `driver made a`, `made a mistake`\n",
    "- four-grams: `The driver made a`, `driver made a mistake`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create _n-grams_ but taking advantage of the [NLTK ngram](http://www.nltk.org/_modules/nltk/model/ngram.html) implementation. We will be using the tokenized list `words` created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'car', 'went', 'too', 'fast', 'on', 'the', 'second', 'lap', '.', 'This', 'damaged', 'the', 'tires', '...']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The',), ('car',), ('went',), ('too',), ('fast',), ('on',), ('the',), ('second',), ('lap',), ('.',), ('This',), ('damaged',), ('the',), ('tires',), ('...',)]\n"
     ]
    }
   ],
   "source": [
    "print(list(ngrams(words, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'car'), ('car', 'went'), ('went', 'too'), ('too', 'fast'), ('fast', 'on'), ('on', 'the'), ('the', 'second'), ('second', 'lap'), ('lap', '.'), ('.', 'This'), ('This', 'damaged'), ('damaged', 'the'), ('the', 'tires'), ('tires', '...')]\n"
     ]
    }
   ],
   "source": [
    "print(list(ngrams(words, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'car', 'went'), ('car', 'went', 'too'), ('went', 'too', 'fast'), ('too', 'fast', 'on'), ('fast', 'on', 'the'), ('on', 'the', 'second'), ('the', 'second', 'lap'), ('second', 'lap', '.'), ('lap', '.', 'This'), ('.', 'This', 'damaged'), ('This', 'damaged', 'the'), ('damaged', 'the', 'tires'), ('the', 'tires', '...')]\n"
     ]
    }
   ],
   "source": [
    "print(list(ngrams(words, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And by looking at the output, it's possible to observe that we are getting what we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grams may be used for several things, like extra features in natural language processing classification tasks. Imagine counts of \"very good\" vs. \"very\" and \"good\" individually when doing sentiment analysis, or the difference in the counts of n-grams present in a reference and our hypothesis as a way of calculating similarity between generated texts, and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping it up\n",
    "\n",
    "And you've reached the end of our first notebook, congratulation! Throughout it you've learned the following concepts:\n",
    "\n",
    "* regex\n",
    "* tokenization\n",
    "* stemming\n",
    "* n-grams\n",
    "\n",
    "\n",
    "It may seem overwhelming, but this is the first step on your journey into the NLP world, so keep at it and see you in Part 2!\n",
    "\n",
    "<img src=\"./media/info_everywhere.jpg\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\[1\\] - [RegExr](https://regexr.com/3lvai)\n",
    "\n",
    "\\[2\\] - [Python Module of the Week](https://pymotw.com/2/re/)\n",
    "\n",
    "\\[3\\] - [NLTK Book](https://www.nltk.org/book/)\n",
    "\n",
    "\\[4\\] - [N-grams](https://en.wikipedia.org/wiki/N-gram#n-gram_models)\n",
    "\n",
    "\\[5\\] - [Language Model](https://en.wikipedia.org/wiki/Language_model)\n",
    "\n",
    "\\[6\\] - [Stanford CS124 Language Modeling slides](https://web.stanford.edu/class/cs124/lec/lm2021.pdf)\n",
    "\n",
    "\\[7\\] - [Stemming and lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Regex: [RegexOne](https://regexone.com/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
