{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Word vectors\n",
    "\n",
    "So far we've seen simple feature selection methods, a statistical feature selection approach, dimensionality reduction techniques such as PCA and SVD, but in the last few years, with the rise in popularity of neural networks, a new technique has become the state of the art for representing words in NLP tasks.\n",
    "\n",
    "This technique is commonly referred to as **word vectors** or **word embeddings**, and its inner workings are really simple. It consists of defining a vocabulary and a vector for each word in it with a maximum number of dimensions. Then all the vectors' are found through the use of **neural networks**. In essence, word embeddings try to capture information on a word's meaning and usage. This not only allows us to significantly reduce the number of features fed to our models, but it also allows meaningful and easy representations across datasets, that are transferrable among tasks. \n",
    "\n",
    "Pretty cool, huh?\n",
    "\n",
    "<img src=\"media/what-year-is-this.jpg\" width=\"400\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "[1. Words vectors explained](#1.-Word-vectors-explained)   \n",
    "&emsp;[1.1 Training word vectors](#1.1-Training-word-vectors)   \n",
    "&emsp;[1.2 Pretrained word vectors](#1.2-Pretrained-word-vectors)   \n",
    "[2. Word representations in Spacy](#2.-Word-representations-in-Spacy)   \n",
    "[3. Cosine similarity](#3.-Cosine-similarity)   \n",
    "[4. Word relations](#4.-Word-relations)   \n",
    "[5. Applying word vectors to sentences](#5.-Applying-word-vectors-to-sentences)   \n",
    "[6. NLP practical example](#6.-NLP-practical-example)   \n",
    "[7. Final remarks](#7.-Final-remarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seed=42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Word vectors explained\n",
    "\n",
    "First of all, by now you could be thinking: \"But wait Doc, didn't I get a bunch of vectors before also?\". Why yes, yes you did, Marty. You could consider the columns of the matrix with document-term counts a possible word vector representation. You could even construct a simpler matrix. \n",
    "\n",
    "If you assume a vocabulary of size V, and each word having an index in this vocabulary, a natural representation would be what is called a one-hot encoding, where each word is represented by a vector of size V - the vocabulary size - with the single component corresponding to this word set to 1, and the remaining fields zeroed out.\n",
    "\n",
    "<img src=\"media/one-hot-vec.png\" width=\"300\">\n",
    "\n",
    "\n",
    "We are going in the right direction! But keep in mind that this representation fits in a very large space and we suddenly fall into the pitfalls of high dimensionality. You could think of applying PCA or SVD to these one-hot vectors but as for most tasks nowadays, neural networks have proven to be better at this. To put it simply, there is a more elegant way. \n",
    "\n",
    "<img src=\"media/but-how-doc.jpg\" width=\"450\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Training word vectors\n",
    "\n",
    "So you know the data - a bunch of words. You know the goal - a vector with an arbitrary number of K features. And you know the means - neural networks. So how does it all work? \"You shall know a word by the company it keeps\". These are the words of John Rupert Firth (at least according to Wikipedia), and they are the basis of the following method - Word2vec. \n",
    "\n",
    "**Word2vec** is a popular technique for using neural models to produce word embeddings, and it encompasses two main approaches - Continuous Bag Of Words (CBOW) and skipgram - that we will describe here.\n",
    "\n",
    "Initially, we center a window of length n around each word in our training text. The word at the center is called the center word and the rest are context words. Each window will produce a training example that we will plug into a neural network. There are two approaches to the training:\n",
    "\n",
    "1 - **CBOW**: the input words are the context words, and we predict the center word \n",
    "\n",
    "2 - **Skip-gram**: complementary to the previous method, the input is the center word, and the predictions are the context words\n",
    "\n",
    "The values we are trying to predict are called the **weights** of the neural network, and they will map to our word vectors directly. We aren't going to deepdive into neural networks at the moment, and there are definitely more details on how to set up these models, but the basic intuition can be seen in the following image:\n",
    "\n",
    "<img src=\"./media/word2vec.png\" width=\"500\">\n",
    "\n",
    "The projection layer depicted contains these **weights** we mentioned, which we'll iteratively train based on a large amount of data so that we get strong word vectors at the end of training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Pretrained word vectors\n",
    "\n",
    "The best thing about these vectors, however, is that they are universal and ready to use of the shelf. They were trained on a huge amount of text data in the same language and we just take them and use them for our task. It saves us the time and effort of gathering, processing and training on our own data.\n",
    "\n",
    "One set of such pretrained vectors is from **spacy**. [Spacy](https://spacy.io) is a toolkit similar to NLTK, but it contains embedded deep learning models for NLP and it typically has better performance for industrial applications. The pretrained word vectors are easy to use out of the box by importing the spacy library.\n",
    "Spacy has several library versions with different vocabulary and vector size, we are loading the medium one. You can try to switch between versions in the following experiments and see the impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous cell throws an error, the library has not downloaded during the virtual environment creation, so you need to do it manually by running `python -m spacy download en_core_web_md` in your terminal.\n",
    "\n",
    "There are several available libraries of word vectors out there, such as [FastText](https://fasttext.cc) and [Glove](https://nlp.stanford.edu/projects/glove/). These all provide good quality embeddings for your NLP tasks. Their training methods are usually based on the Word2vec, but there are differences in the details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word representations in Spacy\n",
    "\n",
    "Now let's dig into the vectors and see what we can get from them. We can start by seeing the representation for a particular word, for example *house*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.9847e-01,  1.8087e-01, -8.9119e-02, -2.5626e-01,  7.4104e-02,\n",
       "        5.9422e-03, -8.0814e-02, -8.7499e-01,  1.6353e-01,  2.7836e+00,\n",
       "       -8.9134e-01,  3.7017e-02, -5.5995e-01, -2.1853e-01, -3.6847e-01,\n",
       "        4.2609e-01,  2.5508e-02,  1.1834e+00, -5.9869e-02, -1.6261e-02,\n",
       "        3.6331e-01,  1.2664e-01,  3.1424e-01,  2.3845e-02,  5.7331e-02,\n",
       "       -4.7905e-01, -2.3247e-01,  2.3379e-02, -2.9739e-01,  1.0735e-01,\n",
       "        2.9723e-01,  5.4123e-02, -2.6837e-01,  4.8272e-01, -4.8055e-02,\n",
       "       -1.0766e-02,  1.6169e-01, -7.4395e-02,  1.2789e-03, -6.1155e-02,\n",
       "        2.4258e-01,  1.4165e-02,  8.3789e-02, -3.5793e-01, -4.8655e-02,\n",
       "        1.1436e-01,  2.7535e-01, -9.2720e-01,  3.2332e-01,  1.6197e-01,\n",
       "       -2.6260e-01, -3.2542e-01,  1.8347e-01,  5.7849e-01,  1.9925e-01,\n",
       "       -3.7611e-01,  1.8520e-01,  1.3349e-01,  1.9571e-01,  5.1844e-01,\n",
       "        2.0733e-01,  2.0470e-01,  8.3850e-02,  4.2725e-01,  1.1571e-01,\n",
       "       -1.2066e-01, -7.6344e-02,  2.2959e-01, -1.9066e-01,  2.8804e-01,\n",
       "       -4.8705e-02, -1.6430e-01, -9.8883e-02, -3.7394e-01,  3.3152e-02,\n",
       "        4.5618e-02,  2.8564e-01, -3.3728e-01, -2.8675e-01, -2.8868e-01,\n",
       "       -1.8131e-01,  6.1240e-01,  1.6237e-01, -2.2660e-01, -3.7512e-02,\n",
       "        2.1622e-01,  4.1143e-01,  4.1526e-01,  4.8579e-01,  3.4196e-02,\n",
       "        1.1097e-02, -4.2380e-02, -5.8883e-02, -2.5180e-01, -1.1827e-01,\n",
       "       -6.5252e-02,  4.3805e-01,  4.2373e-01, -3.5036e-01,  2.0297e-01,\n",
       "       -9.6745e-02,  5.7277e-01, -1.0347e-01,  3.8731e-02,  1.0371e-01,\n",
       "       -4.1211e-01,  1.6263e-01,  9.9873e-02, -1.9250e-02, -3.4607e-01,\n",
       "       -1.3904e-01,  9.3622e-02,  1.1864e-01, -1.5903e-02,  4.8909e-01,\n",
       "        2.2709e-01, -9.8584e-02,  2.4182e-01,  2.1903e-02,  2.8252e-01,\n",
       "       -2.1217e-01,  2.7143e-01, -2.4541e-01, -5.2238e-01,  2.2886e-01,\n",
       "       -2.6915e-01, -7.0856e-02, -1.4449e-01,  1.7106e-01,  4.7452e-02,\n",
       "       -1.5870e-01,  1.8143e-01, -1.6997e-02, -1.5448e-01,  4.7205e-01,\n",
       "        3.1468e-01,  2.3606e-01,  3.5544e-01,  4.6862e-01, -1.5596e-01,\n",
       "       -1.7340e+00, -1.9391e-01, -2.3963e-02, -3.6020e-01,  4.6535e-02,\n",
       "        2.5813e-01, -9.3215e-02,  2.6950e-01,  2.6694e-01, -1.5030e-01,\n",
       "       -2.0352e-02,  1.8663e-01, -1.9440e-01,  2.3475e-01,  3.4072e-01,\n",
       "       -7.1621e-02,  2.2388e-01, -2.3374e-01,  1.4200e-02, -1.4635e-01,\n",
       "        6.6203e-02, -1.3022e-01, -1.9029e-01,  2.0700e-01,  2.0488e-01,\n",
       "       -5.5457e-01,  3.3125e-01,  1.8895e-01,  6.0863e-01,  2.4981e-01,\n",
       "       -2.1294e-01,  2.0438e-01, -2.1388e-01, -9.2709e-03, -3.2134e-01,\n",
       "       -2.7341e-01, -5.3715e-02,  4.4432e-01, -5.6145e-02,  1.9701e-01,\n",
       "        4.9993e-01, -7.0837e-01,  1.6883e-01, -2.1141e-01, -9.5209e-02,\n",
       "       -2.9795e-02, -7.8936e-02, -2.7381e-01,  3.5855e-01, -1.7869e-01,\n",
       "       -4.1047e-01,  4.8600e-02,  2.7462e-02,  1.1432e-01, -7.6645e-02,\n",
       "        3.2180e-01, -2.4885e-01, -4.2627e-01, -1.6614e-01,  1.6740e-01,\n",
       "       -2.1145e-01,  1.7912e-01,  3.7599e-01, -5.1378e-01,  7.2899e-02,\n",
       "       -1.3659e-01,  1.1925e-01,  3.0539e-02,  2.2776e-01, -2.6466e-01,\n",
       "       -2.8589e-01,  2.8825e-02,  2.5696e-01, -4.6584e-02,  2.1268e-01,\n",
       "       -2.8677e-01,  2.7728e-01, -5.6491e-02, -1.7809e-01,  3.4237e-01,\n",
       "       -5.7061e-02, -6.0279e-02, -1.2577e-01,  1.3695e-01,  1.9769e-01,\n",
       "        1.6630e-01, -2.5674e-01,  1.7000e-01, -3.5881e-01, -3.2292e-01,\n",
       "        3.8045e-01,  6.2803e-02, -2.2209e-01,  2.9701e-01,  5.6837e-02,\n",
       "       -2.7707e-01,  1.1020e-01, -3.1815e-01, -4.6311e-02, -2.8384e-01,\n",
       "       -3.4146e-01,  1.1606e-01,  4.3806e-02,  5.8888e-01, -2.2216e-01,\n",
       "       -2.2103e-01,  3.6901e-01, -5.0477e-01, -1.3206e-01, -5.7208e-01,\n",
       "       -2.5279e-01,  5.1948e-02,  4.1786e-01, -1.3912e-01,  1.7719e-01,\n",
       "        4.1387e-01,  3.6797e-01, -2.6381e-01,  1.4578e-01, -3.0316e-01,\n",
       "       -9.4649e-02,  6.3837e-02,  1.3826e-01, -1.8213e-01,  1.7888e-01,\n",
       "       -1.8555e-01, -1.9941e-01, -1.5132e-01, -1.1393e+00,  1.5898e-01,\n",
       "       -2.9225e-01,  2.0079e-01, -4.9275e-02, -8.0235e-01,  1.2834e-02,\n",
       "       -8.9354e-02, -3.0374e-01,  5.6119e-01, -2.0220e-02,  2.9735e-02,\n",
       "        5.8468e-01, -1.0082e-01, -4.7442e-01, -1.2492e-03, -1.7756e-01,\n",
       "        3.0101e-01,  5.8639e-01,  1.2706e-01, -4.8098e-01, -9.8582e-02,\n",
       "       -2.7866e-01, -3.8891e-01,  5.3706e-02,  7.9971e-01, -3.8533e-01,\n",
       "        2.8433e-01,  3.5182e-02, -2.4263e-01, -3.5183e-02, -2.9661e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp('house').vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a simple function just to make it easier and avoid rewriting the same thing over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec(s):\n",
    "    return nlp.vocab[s].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check the size of the vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp('house').vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These word embeddings are 300-dimensional, or, in other words, they have 300 features. We'll come back to this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cosine similarity\n",
    "\n",
    "We can check similarities between words using cosine similarity. The cosine similarity is a measure of similarity between the vectors expressed as the cosine of the angle between them. It is defined by the following equation:\n",
    "\n",
    "$$\\text{cos-similarity} = \\frac{A \\cdot B}{\\| A \\| \\| B \\|}$$\n",
    "\n",
    "More similar vectors point to a similar direction, so the angle between them is low and the cosine similarity is high. The values of cosine similarity are between -1 and 1. At 1, the vectors are pointing in the same direction, at 0, they are perpendicular, and at -1, they are pointing in opposite direction. It is very easy to see this in the 2D plane.\n",
    "\n",
    "<img src=\"./media/cosine.png\" width=\"400\">\n",
    "\n",
    "In this example, there are three animals with two features - if the animal lives in the woods and how much it hunts. The vectors represent where each animal is in this feature space and so if the vectors are closer together, they are more similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity\n",
    "def cosine(v1, v2):\n",
    "    if norm(v1) > 0 and norm(v2) > 0:\n",
    "        return dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out. Using cosine similarity, closer words - like *house* and *home* - should have higher scores. On the other hand words with different meanings, even if they are close in terms of characters - like *house* and *mouse* - should produce a low score, if our word vectors really capture meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7388625"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(vec('house'), vec('home'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16095261"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(vec('house'), vec('mouse'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, *house* is closer to *home* than it is to *mouse*. Makes sense!\n",
    "\n",
    "<img src=\"./media/future.jpg\" width=\"400\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Once again, to simplify our next examples, let's create a function that gets us the closest words to the vector that we are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_closest(token_list, vec_to_check, n=10, dont_include_list=[]):\n",
    "    return sorted([(x, cosine(vec_to_check, vec(x))) for x in token_list if x not in dont_include_list],\n",
    "                  key=lambda x: x[1],\n",
    "                  reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to apply this function in further examples. To simplify a bit, let's limit the vocabulary to the one from the Twitter dataset we used in the previous learning notebook. We can then find the closest words to the word *house*.  We start by reading the dataset and getting its vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/twitter_rep_dem_data_small.csv')\n",
    "\n",
    "handle_removal = lambda doc: re.subn(r'@\\w+','', doc.lower())[0]\n",
    "df['Tweet'] = df['Tweet'].map(handle_removal)\n",
    "\n",
    "simple_tokenizer = lambda doc: \" \".join(WordPunctTokenizer().tokenize(doc))\n",
    "df['Tweet'] = df['Tweet'].map(simple_tokenizer)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit_transform(df.Tweet)\n",
    "\n",
    "tweet_vocab = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the 10 closest words to *house*: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hous', 1.0000001),\n",
       " ('housed', 0.754575),\n",
       " ('houses', 0.754575),\n",
       " ('home', 0.7388625),\n",
       " ('apartment', 0.7146356),\n",
       " ('residence', 0.6567791),\n",
       " ('homes', 0.64025176),\n",
       " ('mansion', 0.62431234),\n",
       " ('room', 0.6134708),\n",
       " ('pantry', 0.6085841)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(tweet_vocab.keys(),\n",
    "              vec('house'),\n",
    "              dont_include_list=['house'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word relations\n",
    "\n",
    "There is much more that we can do to show you that these vectors capture the meaning, or at least some semantic information, of our vocabulary. Hopefully, if you still don't believe it, this will help. For example, what do you think will happen if we subtract man from king and add woman?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.78808445),\n",
       " ('prince', 0.6401078),\n",
       " ('princess', 0.61256355),\n",
       " ('royal', 0.580097),\n",
       " ('throne', 0.57870126),\n",
       " ('queens', 0.5743794),\n",
       " ('kingdom', 0.5520981),\n",
       " ('lady', 0.5254389),\n",
       " ('woma', 0.5150813),\n",
       " ('mother', 0.49758324)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(tweet_vocab.keys(), \n",
    "              vec('king') - vec('man') + vec('woman'),\n",
    "              dont_include_list=['king', 'man', 'woman'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/mind-blown-2.png\" width=\"300\">\n",
    "\n",
    "\n",
    "And what is the mean between morning and evening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('afternoon', 0.92605567),\n",
       " ('night', 0.82258904),\n",
       " ('mornings', 0.7452829),\n",
       " ('noon', 0.7396107),\n",
       " ('10am', 0.7396107),\n",
       " ('2pm', 0.7396107),\n",
       " ('11am', 0.7396107),\n",
       " ('4pm', 0.7396107),\n",
       " ('8am', 0.7396107),\n",
       " ('5pm', 0.7396107)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(tweet_vocab.keys(),\n",
    "              np.mean(np.array([vec('morning'), vec('evening')]), axis=0),\n",
    "              dont_include_list=['morning', 'evening'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/mind-blown-3.png\" width=\"300\">\n",
    "\n",
    "\n",
    "What the sky is to blue, the grass is to ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('green', 0.6426651),\n",
       " ('red', 0.5832038),\n",
       " ('purple', 0.5736931),\n",
       " ('lilac', 0.5736931),\n",
       " ('lawn', 0.5727839),\n",
       " ('landscaping', 0.5727839),\n",
       " ('turf', 0.54457676),\n",
       " ('orange', 0.53865606),\n",
       " ('brown', 0.5230748),\n",
       " ('hazel', 0.5230748)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(tweet_vocab.keys(), \n",
    "              vec('blue') - vec('sky') + vec('grass'),\n",
    "              dont_include_list=['blue', 'sky', 'grass'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/mind-blown-4.png\" width=\"300\">\n",
    "\n",
    "<br>\n",
    "\n",
    "## 5. Applying word vectors to sentences\n",
    "\n",
    "There are several ways you could think of to construct a sentence representation from these vectors, such as:\n",
    "\n",
    "* sum\n",
    "* average \n",
    "* concatenation\n",
    "\n",
    "The average is a good enough approach to start with, so let's implement a function to get the sentence vector representation from the average of its words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentvec(s):\n",
    "    sent = nlp(s)\n",
    "    return np.mean(np.array([w.vector for w in sent]), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the same logic to get the closest sentence according to the sentence representation we chose. Below you have the implementation of the previous function that used cosine similarity, but for sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_closest_sent(space, input_str, n=10):\n",
    "    input_vec = sentvec(input_str)\n",
    "    return sorted(space,\n",
    "                  key=lambda x: cosine(sentvec(x), input_vec),\n",
    "                  reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out with a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last night , i voted against advancing the republican tax bill because i ' ve heard from delawareans about the need fo … https :// t . co / sseek7kbaq\n",
      "---\n",
      "congressional republicans and the trump administration must realize that now is the time for progress – not rollbacks . # earthday\n",
      "---\n",
      "i call on mr . pruitt to resign because americans cannot and should not be the victim of his abuse of power .\n",
      "---\n",
      "rt : the court is giving the trump administration 90 days to explain why they ended # daca , which the administrati …\n",
      "---\n",
      "i am opposed to this proposal and will fight to keep fair and just .\n",
      "---\n",
      "rt : a civil war that gets so little attention from u . s . public . in congress it has become about the proxy war between iran and saud …\n",
      "---\n",
      "this week , i joined 73 of my colleagues in calling on the administration to stop playing politics with the cdc . the … https :// t . co / gqr1cesbar\n",
      "---\n",
      "“ stop donald trump and his politics of fear .” - i support this message from the brave group of migrants at the u . s .… https :// t . co / dfresk6nou\n",
      "---\n",
      "a5 : the decision was disappointing , to say the least . i believe the united states has a moral obligation to addres … https :// t . co / u78g6a35gy\n",
      "---\n",
      "rt : when i was in college , students were able to graduate without crushing debt . unfortunately , that is not the reality for man …\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for sent in spacy_closest_sent(df.Tweet.values[:2000], \"i am against the trump administration .\"):\n",
    "    print(sent)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to have worked quite well, wouldn't you agree, Marty?\n",
    "\n",
    "If you are still not convinced about this, you can try to project all your vectors into a 2D space (by applying PCA, for example) and convince yourself that words are somewhat organized by meaning, and we can extract word relations from its distances. If you project your vectors, you should get something similar to this:\n",
    "\n",
    "<img src=\"./media/word-vectors-projection.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. NLP practical example\n",
    "\n",
    "All that is left is to use these new word representations as the features of our models. We start by defining a function to build sentence vectors for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sentence_vecs(docs):\n",
    "    num_examples = len(docs)\n",
    "    word_vector_shape = nlp.vocab.vectors.shape[-1]\n",
    "    vectors = np.zeros([num_examples, word_vector_shape])\n",
    "    for ii, doc in enumerate(docs):\n",
    "        vector = sentvec(doc)\n",
    "        vectors[ii] = vector\n",
    "    \n",
    "    # in case we get any NaN's or Inf, replace them with 0s\n",
    "    return np.nan_to_num(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's get a baseline as we did before (it should match the one from the previous notebook). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5062994960403168\n"
     ]
    }
   ],
   "source": [
    "handle_removal = lambda doc: re.subn(r'@\\w+','', doc.lower())[0]\n",
    "df['Tweet'] = df['Tweet'].map(handle_removal)\n",
    "\n",
    "simple_tokenizer = lambda doc: \" \".join(WordPunctTokenizer().tokenize(doc))\n",
    "df['Tweet'] = df['Tweet'].map(simple_tokenizer)\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.3, random_state=seed)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data.Tweet)\n",
    "X_test = vectorizer.transform(test_data.Tweet)\n",
    "\n",
    "y_train = train_data.Party\n",
    "y_test = test_data.Party\n",
    "\n",
    "clf =  KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy: {}'.format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also get baselines for our previous methods - SVD and PCA. We'll use 300 as the number of components to keep so we can compare with the new technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncated SVD Accuracy: 0.6040316774658028\n",
      "PCA Accuracy: 0.599712023038157\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data.Tweet)\n",
    "X_test = vectorizer.transform(test_data.Tweet)\n",
    "\n",
    "svd = TruncatedSVD(n_components=300, random_state=seed)\n",
    "svd.fit(X_train)\n",
    "X_train_svd = svd.transform(X_train)\n",
    "X_test_svd =  svd.transform(X_test)\n",
    "\n",
    "clf =  KNeighborsClassifier()\n",
    "clf.fit(X_train_svd, y_train)\n",
    "y_pred = clf.predict(X_test_svd)\n",
    "print('Truncated SVD Accuracy: {}'.format(accuracy_score(y_pred, y_test)))\n",
    "\n",
    "pca = PCA(n_components=300, random_state=seed)\n",
    "X_train_dense = X_train.toarray()\n",
    "X_test_dense = X_test.toarray()\n",
    "pca.fit(X_train_dense)\n",
    "X_train_pca = pca.transform(X_train_dense)\n",
    "X_test_pca =  pca.transform(X_test_dense)\n",
    "\n",
    "clf =  KNeighborsClassifier()\n",
    "clf.fit(X_train_pca, y_train)\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "print('PCA Accuracy: {}'.format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 300 features, PCA and SVD have a pretty low accuracy. Now let's split the data and build the vectors - it might take a few minutes to get the vectors for all training and test data. We'll print the shape of the output vector - you should see that our feature vector now has 300 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12963, 300)\n"
     ]
    }
   ],
   "source": [
    "#calculate sentence vectors for each tweet\n",
    "X_train = build_sentence_vecs(train_data.Tweet.values)\n",
    "X_test = build_sentence_vecs(test_data.Tweet.values)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the same model and see how much accuracy we can get out of our 300 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6385889128869691\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 300 features, this is a pretty close accuracy. We can even go further, for example let's try to remove stopwords from the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to download the set of stopwords. Uncomment the below line to run `nltk.download('stopwords')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/catarinasilva/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6580273578113751\n"
     ]
    }
   ],
   "source": [
    "# Redefine functions to use stopwords information\n",
    "\n",
    "def sentvec_tfidf(s, stopwords):\n",
    "    sent = nlp(s)\n",
    "    return np.average(np.array([w.vector for w in sent if w.text not in stopwords]), axis=0)\n",
    "    \n",
    "def build_sentence_vecs_tfidf(docs, stopwords):\n",
    "    num_examples = len(docs)\n",
    "    word_vector_shape = nlp.vocab.vectors.shape[-1]\n",
    "    vectors = np.zeros([num_examples, word_vector_shape])\n",
    "    for ii, doc in enumerate(docs):\n",
    "        vector = sentvec_tfidf(doc, stopwords)\n",
    "        vectors[ii] = vector\n",
    "    \n",
    "    # in case we get any NaN's or Inf, replace them with 0s\n",
    "    return np.nan_to_num(vectors)\n",
    "\n",
    "# Run with english stopwords\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "X_train = build_sentence_vecs_tfidf(train_data.Tweet.values, stopwords)\n",
    "X_test = build_sentence_vecs_tfidf(test_data.Tweet.values, stopwords)\n",
    "\n",
    "clf =  KNeighborsClassifier()\n",
    "clf.fit(X_train, train_data.Party)\n",
    "pred = clf.predict(X_test)\n",
    "print('Accuracy: {}'.format(accuracy_score(pred, test_data.Party)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We got a little bit more accuracy with a simple strategy. There are much more tweaks that you can (and should) try to improve your accuracy. \n",
    "\n",
    "<br> \n",
    "\n",
    "## 7. Final remarks\n",
    "\n",
    "In this last part, we've shown you that word vectors are pretty useful and intuitive, keeping meaningful information about words in a compact feature space. If you wish to dig further into these word representations we suggest this [paper](https://arxiv.org/pdf/1301.3781.pdf). As before, take into consideration that although word vectors can be used as an out of the box solution for several NLP tasks, all the factors mentioned before will affect your model performance. So once again, you should still be careful with:\n",
    "\n",
    "- Initial text preprocessing\n",
    "- Choice of classifier\n",
    "- Parameter selection\n",
    "\n",
    "In particular, for most of NLP tasks, neural networks have been showing extremely good performance, and if you really want to get into this field, you should learn more about that. However, these basic techniques are essential to understand some of the reasoning when handling text and can still prove quite useful to us.\n",
    "\n",
    "And that's it for this BLU. You have come out on the other side with a much wider view of the different methods and reasoning you can take when handling features in NLP (and outside NLP) in a high dimensional space. There is so much more, but these basic tools should suffice for you to start working with text data and to understand more complex approaches built on top of these methods. See you in the next BLU!\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./media/see-you-in-the-future.png\" width=\"500\">\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
