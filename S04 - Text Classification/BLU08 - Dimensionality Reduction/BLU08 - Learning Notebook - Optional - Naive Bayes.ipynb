{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "576afa4a",
   "metadata": {},
   "source": [
    "# Naïve Bayes classifier - An introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7fa37c",
   "metadata": {},
   "source": [
    "## The \"Bayes\" part\n",
    "\n",
    "In the Naive Bayes classifier, the Bayes part comes from the Bayes theorem, and a link to Bayesian statistics. Now, we won't deep dive into the details of Bayesian statistics [1] but you can take the following principle from it: the Bayesian approach to probability allows you to incorporate prior information or expectations you have about your data and update them as you see new observations, to generate a final expectation over your hypothesis.   \n",
    "\n",
    "Unlike frequentist approaches, which assume that your data was generated randomly from some distribution with fixed parameters - and these are the parameters you attempt to estimate - Bayes approaches assume the parameters itself to not be fixed, and instead to be generated from a certain distribution themselves. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc6ff0b",
   "metadata": {},
   "source": [
    "### Bayes’ Theorem\n",
    "\n",
    "The Bayes theorem, which you may have seen before, is a way of representing the conditional probability of two events A and B.\n",
    "\n",
    "First, let's define some variables:\n",
    "\n",
    "* $P(A)$: probability of event A ocurring\n",
    "* $P(B)$: probability of event B ocurring\n",
    "* $P(A|B)$: probability of event A ocurring knowing that B ocurred\n",
    "* $P(B|A)$: probability of event B ocurring knowing that A ocurred\n",
    "* $P(A\\cap B)$: probability of event A and event B ocurring together, i.e., the joint probability of both events\n",
    "\n",
    "Now, taking from the conditional probabilities, we know that:\n",
    "\n",
    "$$P(A\\cap B) = P(A|B)P(B) $$ \n",
    "\n",
    "In the same way:\n",
    "\n",
    "$$P(A\\cap B) = P(B|A)P(A) $$ \n",
    "\n",
    "From this, the Bayes theorem derives the following:\n",
    "\n",
    "$$ P(A|B)P(B) = P(B|A)P(A)  \\Leftrightarrow  P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$ \n",
    "\n",
    "Now, if you think of event A as a hypothesis we're interested in testing, and B as the data we observe, we could write the following:\n",
    "\n",
    "$$ P(hyp|data) = \\frac{P(data|hyp)P(hyp)}{P(data)} $$ \n",
    "\n",
    "Going a bit further with this you can describe:\n",
    "\n",
    "* $P(hyp)$: our **prior** knowledge about the hypothesis\n",
    "* $P(data)$: overal probability of our data, independent of any hypothesis\n",
    "* $P(data|hyp)$: probability of this data ocurring if our hypothesis is true, usually called the **likelihood** of our data\n",
    "* $P(hyp|data)$: how likely our hypothesis is to be true given the data we're observing, usually called the **posterior** probability\n",
    "\n",
    "$P(hyp)$ can be seen as your **prior** belief in our hypothesis, while $P(hyp|data)$ can be seen as an updated belief, or **posterior** belief after observing some data.\n",
    "\n",
    "\n",
    "![seashell](media/seashell.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5c7e22",
   "metadata": {},
   "source": [
    "### The \"Naïve\" part\n",
    "\n",
    "Now that you got the gist of Bayes theorem, let's dive into why it's called naïve. The theorem we presented above seems quite simple right?\n",
    "\n",
    "$$ P(hyp|data) = \\frac{P(data|hyp)P(hyp)}{P(data)} $$ \n",
    "\n",
    "But if you take a closer look, the overal notion of observed `data` can be quite complex. This is, by data, we usually are refering to all the events that we are using to try to predict the probability of our hypothesis. _Do you see where I'm going with this?_\n",
    "\n",
    "That's right, I'm talking about features. And if we expand that to what we typically have - multiple features - then each feature can be seen as one event, and thus we can rewrite the equation as follows:\n",
    "\n",
    "\n",
    "$$ P(hyp|\\{x_1 \\cap x_2 \\cap ... \\cap x_n\\}) = \\frac{P(\\{x_1 \\cap x_2, ... \\cap x_n\\}|hyp)P(hyp)}{P(\\{x_1 \\cap x_2 \\cap ... \\cap x_n\\})} $$ \n",
    "\n",
    "\n",
    "This is where the naïve part comes in. We are going to make an assumption, and a pretty big one. We're going to assume that our features are independent of each other. With this **independence assumption** we get the following:\n",
    "\n",
    "$$ P(x_1 \\cap x_2, ... \\cap x_n) = P(x_1)P(x_2) ... P(x_n) $$\n",
    "\n",
    "which is also valid for the conditional probability\n",
    "\n",
    "$$ P(x_1 \\cap x_2, ... \\cap x_n | hyp) = P(x_1|hyp)P(x_2|hyp) ... P(x_n|hyp) $$\n",
    "\n",
    "Putting all of this together we get:\n",
    "\n",
    "$$ P(hyp|\\{x_1 \\cap x_2 \\cap ... \\cap x_n\\}) = \\frac{P(x_1|hyp)P(x_2|hyp) ... P(x_n|hyp) * P(hyp)}{P(x_1)P(x_2) ... P(x_n))} $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9d0dad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## A simple example\n",
    "\n",
    "Imagine that we have the following hypothesis over a piece of fruit taken from a bowl of fruit. Our hypothesis will be that the piece of fruit is an orange:\n",
    "\n",
    "$H_0$: fruit is an orange\n",
    "\n",
    "And we define some features that we observe \n",
    "\n",
    "* feature 1 ($x_1$): color is orange\n",
    "* feature 2 ($x_2$): shape is round\n",
    "\n",
    "Then by definition:\n",
    "\n",
    "$$ P(H_0|\\{x_1 \\cap x_2\\}) = \\frac{P(x_1|H_0)P(x_2|H_0)P(H_0)}{P(x_1)P(x_2)} $$ \n",
    "\n",
    "\n",
    "### Prior\n",
    "\n",
    "As mentioned before, $P(H_0)$ is our prior belief in the hypothesis. If I only buy oranges and apples, and I buy them in the same quantities, for example, I could say my prior probability that I'll get an orange from my fruit bowl is the same as that - 0.5.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9682f63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eae2ee9",
   "metadata": {},
   "source": [
    "### Likelihoods\n",
    "\n",
    "If I have some previous data that I've seen:\n",
    "\n",
    "| observation | color | shape | is_orange | \n",
    "|---|-------|-------|-------|\n",
    "| 1 | orange  | round | yes | \n",
    "| 2 | orange  | round | yes | \n",
    "| 3 | orange  | round | no | \n",
    "| 4 | red  | round | no | \n",
    "| 5 | green  | round | yes | \n",
    "| 6 | red  | not round | no | \n",
    "\n",
    "Then I can extract my likelihoods from here.\n",
    "\n",
    "$$ P(x_1=orange|H_0) = \\frac{\\#(x_1=orange, H_0=yes)}{\\#(H_0=yes)} $$\n",
    "\n",
    "i.e. the number of times we've seen that the color was orange on all instances that our hypothesis was true. In the same way:\n",
    "\n",
    "$$ P(x_2=round|H_0) = \\frac{\\#(x_2=round, H_0=yes)}{\\#(H_0=yes)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd4c5dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"orange\", \"round\", \"yes\"),\n",
    "    (\"orange\", \"round\", \"yes\"),\n",
    "    (\"orange\", \"round\", \"no\"),\n",
    "    (\"red\", \"round\", \"yes\"),\n",
    "    (\"green\", \"round\", \"yes\"),\n",
    "    (\"red\", \"not round\", \"no\"),\n",
    "]\n",
    "\n",
    "def likelihood(data, feat_index, feat_value, hyp):\n",
    "    \n",
    "    count_hyp = 0\n",
    "    count_feat_hyp = 0\n",
    "    for row in data:\n",
    "        if row[-1] == hyp:\n",
    "            count_hyp += 1\n",
    "            if row[feat_index] == feat_value:\n",
    "                count_feat_hyp += 1\n",
    "    \n",
    "    return 1.0*count_feat_hyp/count_hyp\n",
    "    \n",
    "p_x_1_h_0 = likelihood(data, 0, \"orange\", \"yes\")\n",
    "p_x_2_h_0 = likelihood(data, 1, \"round\", \"yes\")\n",
    "\n",
    "print(p_x_1_h_0)\n",
    "print(p_x_2_h_0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23315bf0",
   "metadata": {},
   "source": [
    "### Evidence\n",
    "\n",
    "Finally, we'll do $P(x_1)$ and $P(x_2). Now, this is slightly less important as you'll see in the following examples, because it's a normalizing constant for multi-class problems. But let's extract it from our dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2577a508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def prob_evidence(data, feat_index, feat_value):\n",
    "    \n",
    "    count_rows = 0\n",
    "    count_feat = 0\n",
    "    for row in data:\n",
    "        count_rows += 1\n",
    "        if row[feat_index] == feat_value:\n",
    "            count_feat += 1\n",
    "    \n",
    "    return 1.0*count_feat/count_rows\n",
    "    \n",
    "p_x_1 = prob_evidence(data, 0, \"orange\")\n",
    "p_x_2 = prob_evidence(data, 1, \"round\")\n",
    "\n",
    "print(p_x_1)\n",
    "print(p_x_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e30bdc",
   "metadata": {},
   "source": [
    "The way we would infer our final posterior probability for the hypothesis we posed follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40cec6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "p_h_0_X = (p_x_1_h_0 * p_x_2_h_0 * prior)/(p_x_1*p_x_2)\n",
    "print(p_h_0_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b766e88",
   "metadata": {},
   "source": [
    "If my prior had been different (let's say that I buy oranges very rarely) we would get a slightly different result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2818022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12\n"
     ]
    }
   ],
   "source": [
    "prior = 0.1\n",
    "p_h_0_X = (p_x_1_h_0 * p_x_2_h_0 * prior)/(p_x_1*p_x_2)\n",
    "print(p_h_0_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c08e6d",
   "metadata": {},
   "source": [
    "The idea that we're combining prior knowledge or beliefs that we have with actual observations should now be more or less obvious to you. The following image from [2] shows you a visual interpretation of this:\n",
    "\n",
    "![bayes_charts](media/bayes_visualization.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6ee01f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Applying Naïve Bayes to text classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6e059a",
   "metadata": {},
   "source": [
    "\n",
    "Now for the interesting part - applying it to text classification!\n",
    "\n",
    "Imagine that we want to classify a particular review as:\n",
    "\n",
    "* C_0: positive review\n",
    "* C_1: negative review\n",
    "* C_2: neutral review\n",
    "\n",
    "According to our formulas we have:\n",
    "\n",
    "$$P(C_i|D_j) = \\frac{P(D_j|C_i)*P(C_i)}{P(D_j)}$$\n",
    "\n",
    "Where: \n",
    "\n",
    "* $P(C_i)$ is the overal probability of the class, it is usually called **prior**;\n",
    "* $P(D_j)$ is the overal probability of the document;\n",
    "* $P(D_j|C_i)$ is the **likelihood** of a document showing up for a particular class.\n",
    "\n",
    "To make things simpler notice that $P(D_j)$ is constant across the two classes, i.e., when comparing among classes, the value doesn't change. Since we're only interested in knowing which class has higher probability, we can just say that $P(C_i|D_j)$ is proportional to the following quantity:\n",
    "\n",
    "$$P(D_j|C_i)*P(C_i)$$\n",
    "\n",
    "And so this is the formula we will use to compute our **comparative probabilities** between classes. \n",
    "\n",
    "Now, a document is a set of words, which we consider our features. Using (and abusing) the independence assumption of naïve bayes, the likelihood of a document showing in a particular class is just the likelihood of its set of words showing up in that class:\n",
    "\n",
    "$$P(D_j|C_i) = P(w_1|C_i) * P(w_2|C_i) * P(w_3|C_i) * ... * P(w_n|C_i)$$\n",
    "\n",
    "And our final formula becomes:\n",
    "\n",
    "$$P(C_i|D_j) = [P(w_1|C_i) * P(w_2|C_i) * P(w_3|C_i) * ... * P(w_n|C_i)]*P(C_i)$$\n",
    "\n",
    "\n",
    "In an analogous way to our previous fruit example, we consider our $P(w_j|C_i)$ the estimators we can take from classes, and $P(C_i)$ our prior on all the classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ed4a5d",
   "metadata": {},
   "source": [
    "So, similar to the example we did above\n",
    "\n",
    "#### Training\n",
    "\n",
    "* estimate $P(w_k|C_i)$ for all words, based on counts of the words in documents\n",
    "* set or estimate $P(C_i)$, either from dataset, previous knowledge or just assigning equal probabilities\n",
    "\n",
    "#### Predicting\n",
    "\n",
    "* Compute $P(C_i|D_j) = [P(w_1|C_i) * P(w_2|C_i) * P(w_3|C_i) * ... * P(w_n|C_i)] * P(C_i)$\n",
    "* Choose the class satisfying $C = argmax_i P(C_i|D_j)$\n",
    "\n",
    "\n",
    "This is exactly what `MultinomialNB` does for you, and the reason why it's a good baseline follows (quoting directly from [3]):\n",
    "\n",
    "> Because of the independence assumption, naive Bayes classifiers can quickly learn to use high dimensional features with limited training data compared to more sophisticated methods. This can be useful in situations where the dataset is small compared to the number of features, such as images or texts.\n",
    "\n",
    "There are other ways of handling the dimensionality curse, as you've seen in this BLU, but now you understand why Naïve Bayes is such an interesting method in these cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985e5b27",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] [MIT OpenCourseWare - Statistics for Applications: 17. Bayesian Statistics](https://www.youtube.com/watch?v=bFZ-0FH5hfs)\n",
    "\n",
    "[2] [What Is Bayesian Inference?](https://towardsdatascience.com/what-is-bayesian-inference-4eda9f9e20a6)\n",
    "\n",
    "[3] https://shuzhanfan.github.io/2018/06/understanding-mathematics-behind-naive-bayes/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
